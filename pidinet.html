<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/1998/REC-html40-19980424/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"><meta name="generator" content="ABBYY FineReader 15">
<style type="text/css">
 table.main {}
 tr.row {}
 td.cell {}
 div.block {}
 div.paragraph {}
 .font0 { font:4pt Arial, sans-serif; }
 .font1 { font:5pt Arial, sans-serif; }
 .font2 { font:6pt Arial, sans-serif; }
 .font3 { font:7pt Arial, sans-serif; }
 .font4 { font:8pt Arial, sans-serif; }
 .font5 { font:11pt Arial, sans-serif; }
 .font6 { font:44pt Arial, sans-serif; }
 .font7 { font:4pt Arial Unicode MS, sans-serif; }
 .font8 { font:6pt Arial Unicode MS, sans-serif; }
 .font9 { font:7pt Arial Unicode MS, sans-serif; }
 .font10 { font:10pt Calibri, sans-serif; }
 .font11 { font:9pt Courier New, monospace; }
 .font12 { font:5pt MS Gothic, monospace; }
 .font13 { font:6pt MS Gothic, monospace; }
 .font14 { font:9pt MS Gothic, monospace; }
 .font15 { font:6pt SimHei, sans-serif; }
 .font16 { font:7pt SimHei, sans-serif; }
 .font17 { font:5pt SimSun, serif; }
 .font18 { font:6pt SimSun, serif; }
 .font19 { font:7pt SimSun, serif; }
 .font20 { font:9pt SimSun, serif; }
 .font21 { font:10pt SimSun, serif; }
 .font22 { font:7pt Times New Roman, serif; }
 .font23 { font:8pt Times New Roman, serif; }
 .font24 { font:9pt Times New Roman, serif; }
 .font25 { font:10pt Times New Roman, serif; }
 .font26 { font:11pt Times New Roman, serif; }
 .font27 { font:14pt Times New Roman, serif; }

</style>
</head>
<body><a name="caption1"></a>
<h2><a name="bookmark0"></a><span class="font27" style="font-weight:bold;">Pixel Difference Networks for Efficient Edge Detection</span></h2>
<p><a name="bookmark1"></a><span class="font26">Zhuo Su</span><span class="font9"><sup>1</sup></span><span class="font12">，</span><span class="font8">* &nbsp;&nbsp;</span><span class="font26">Wenzhe Liu</span><span class="font9"><sup>2</sup></span><span class="font12">,</span><span class="font23">* </span><span class="font26">Zitong Yu</span><span class="font9"><sup>1</sup> &nbsp;&nbsp;</span><span class="font26">Dewen Hu</span><span class="font9"><sup>2</sup> &nbsp;&nbsp;</span><span class="font26">Qing Liao</span><span class="font9"><sup>3</sup> &nbsp;&nbsp;</span><span class="font26">Qi Tian</span><span class="font9"><sup>4</sup></span></p>
<p><span class="font26">Matti Pietikainen</span><span class="font9"><sup>1</sup> &nbsp;&nbsp;&nbsp;</span><span class="font26">Li Liu</span><span class="font9"><sup>2,1</sup>，</span></p>
<p><span class="font9"><sup>1</sup> </span><span class="font26">Center for Machine Vision and Signal Analysis, University of Oulu, Finland</span></p>
<p><span class="font9"><sup>2</sup></span><span class="font26">National University of Defense Technology, China</span></p>
<p><span class="font9"><sup>3</sup></span><span class="font26">Harbin Institute of Technology (Shenzhen), China </span><span class="font9"><sup>4</sup></span><span class="font26">Xidian University, China</span></p>
<p><span class="font24">{</span><span class="font11">zhuo.su, zitong.yu, matti.pietikainen, li.liu</span><span class="font24">}</span><span class="font11">@oulu.fi</span></p>
<p><span class="font24">{</span><span class="font11">liuwenzhe15, dwhu</span><span class="font24">}</span><span class="font11">@nudt.edu.cn, </span><a href="mailto:liaoqing@hit.edu.cn"><span class="font11">liaoqing@hit.edu.cn</span></a><span class="font11">, </span><a href="mailto:wywqtian@gmail.com"><span class="font11">wywqtian@gmail.com</span></a></p>
<div>
<h3><a name="bookmark2"></a><span class="font26" style="font-weight:bold;">Abstract</span></h3>
<p><span class="font25" style="font-style:italic;">Recently, deep Convolutional Neural Networks (CNNs) can achieve human-level performance in edge detection with the rich and abstract edge representation capacities. However, the high performance of CNN based edge detection is achieved with a large pretrained CNN backbone, which is memory and energy consuming. In addition, it is surprising that the previous wisdom from the traditional edge detectors, such as Canny, Sobel, and LBP are rarely investigated in the rapid-developing deep learning era. To address these issues, we propose a simple, lightweight yet effective architecture named Pixel Difference Network (PiDiNet)for efficient edge detection. PiDiNet adopts novel pixel difference convolutions that integrate the traditional edge detection operators into the popular convolutional operations in modern CNNs for enhanced performance on the task, which enjoys the best of both worlds. Extensive experiments on BSDS500, NYUD, and Multicue are provided to demonstrate its effectiveness, and its high training and inference efficiency. Surprisingly, when training from scratch with only the BSDS500 and VOC datasets, PiDiNet can surpass the recorded result of human perception (0.807 vs. 0.803 in ODS F-measure) on the BSDS500 dataset with 100 FPS and less than 1M parameters. A faster version of PiDiNet with less than 0.1M parameters can still achieve comparable performance among state of the arts with 200 FPS. Results on the NYUD and Multicue datasets show similar observations. The codes are available at</span><a href="https://github.com/zhuoinoulu/pidinet"><span class="font25" style="font-style:italic;"> https://github.com/zhuoinoulu/pidinet.</span></a></p>
<h3><a name="bookmark3"></a><span class="font26" style="font-weight:bold;"><a name="bookmark4"></a>1. Introduction</span></h3>
<p><span class="font25">Edge detection has been a longstanding, fundamental low-level problem in computer vision </span><a href="#bookmark5"><span class="font25">[5]</span></a><span class="font25">. Edges and object</span></p>
<p><span class="font8">*</span><span class="font23">Equal contributions. </span><span class="font8">f </span><span class="font23">Corresponding author:</span><a href="http://lilyliliu.com"><span class="font23"> http://lilyliliu.com</span></a></p>
</div><br clear="all">
<div>
<p><span class="font1" style="font-weight:bold;">Roberts operator</span></p>
<p><span class="font1" style="font-weight:bold;">Sobel operator</span></p><img src="pidinet_files/pidinet-1.jpg" alt="" style="width:27pt;height:57pt;">
</div><br clear="all">
<div>
<p><span class="font1" style="font-weight:bold;">LoG operator</span></p><img src="pidinet_files/pidinet-2.jpg" alt="" style="width:27pt;height:26pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-3.jpg" alt="" style="width:27pt;height:57pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-4.jpg" alt="" style="width:27pt;height:26pt;">
</div><br clear="all">
<div>
<p><span class="font1" style="font-weight:bold;">(a) Kernels in well-designed traditional operators for edge detection</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-5.jpg" alt="" style="width:52pt;height:42pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-6.jpg" alt="" style="width:39pt;height:27pt;">
<p><span class="font2">-1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1</span></p>
</div><br clear="all">
<div>
<p><span class="font1" style="font-weight:bold;">(b) CNNs kernels which are semantically strong while hard to give emphasis on edge information (Fig2, top row)</span></p><img src="pidinet_files/pidinet-7.jpg" alt="" style="width:101pt;height:13pt;">
<p><span class="font1" style="font-weight:bold;">(c) PDC benefiting from both worlds (Fig2, bottom row)</span></p>
<p><span class="font24">Figure 1. PDC benefits from both worlds with proper integration of traditional operators and modern CNNs.</span></p>
</div><br clear="all">
<div>
<p><span class="font25">boundaries play an important role in various higher-level computer vision tasks such as object recognition and detection </span><a href="#bookmark6"><span class="font25">[29,</span></a><a href="#bookmark7"><span class="font25"> 11]</span></a><span class="font25">, object proposal generation </span><a href="#bookmark8"><span class="font25">[6,</span></a><a href="#bookmark9"><span class="font25"> 54]</span></a><span class="font25">, image editing </span><a href="#bookmark10"><span class="font25">[10]</span></a><span class="font25">, and image segmentation </span><a href="#bookmark11"><span class="font25">[41,</span></a><a href="#bookmark12"><span class="font25"> 4]</span></a><span class="font25">. Therefore, recently, the edge detection problem has also been revisited and injected new vitality due to the renaissance of deep learning </span><a href="#bookmark13"><span class="font25">[2,</span></a><a href="#bookmark14"><span class="font25"> 23,</span></a><a href="#bookmark15"><span class="font25"> 47,</span></a><a href="#bookmark16"><span class="font25"> 60,</span></a><a href="#bookmark17"><span class="font25"> 55,</span></a><a href="#bookmark18"><span class="font25"> 31]</span></a><span class="font25">.</span></p>
<p><span class="font25">The main goal of edge detection is identifying sharp image brightness changes such as discontinuities in intensity, color, or texture </span><a href="#bookmark19"><span class="font25">[53]</span></a><span class="font25">. Traditionally, edge detectors based on image gradients or derivatives information are popular choices. Early classical methods use the first or second order derivatives (</span><span class="font25" style="font-style:italic;">e.g.</span><span class="font25">, Sobel </span><a href="#bookmark20"><span class="font25">[50]</span></a><span class="font25">, Prewitt </span><a href="#bookmark21"><span class="font25">[46]</span></a><span class="font25">, Laplacian of Gaussian (LoG), Canny </span><a href="#bookmark5"><span class="font25">[5]</span></a><span class="font25">, </span><span class="font25" style="font-style:italic;">etc.)</span><span class="font25"> for basic edge detection. Later learning based methods </span><a href="#bookmark22"><span class="font25">[16,</span></a><a href="#bookmark23"><span class="font25"> 9]</span></a><span class="font25"> further utilize various gradient information </span><a href="#bookmark24"><span class="font25">[59,</span></a><a href="#bookmark25"><span class="font25"> 37,</span></a><a href="#bookmark26"><span class="font25"> 12,</span></a><a href="#bookmark27"><span class="font25"> 15]</span></a><span class="font25"> to produce more accurate boundaries.</span></p>
<p><span class="font25">Due to the capability of automatically learning rich representations of data with hierarchical levels of abstraction, deep CNNs have brought tremendous progress for various computer vision tasks including edge detection and are still rapidly developing. Early deep learning based edge detection models construct CNN architectures as classifiers to predict the edge probability of an input image</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-8.jpg" alt="" style="width:485pt;height:72pt;">
<p><span class="font3">Feature maps generated by pixel difference convolution</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-9.jpg" alt="" style="width:379pt;height:72pt;">
<p><a name="bookmark28"></a><span class="font24">Figure 2. PiDiNet configured with pixel difference convolution (PDC) </span><span class="font24" style="font-style:italic;">vs.</span><span class="font24"> the baseline with vanilla convolution. Both models were trained only using the BSDS500 dataset. Compared with vanilla convolution, PDC can better capture gradient information from the image that facilitates edge detection.</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-10.jpg" alt="" style="width:47pt;height:71pt;">
</div><br clear="all">
<p><a name="bookmark29"></a><span class="font24">Table 1. Comparison between ours and some leading edge detection models in terms of efficiency and accuracy. The multiply-accumulates (MACs) are calculated based on a 200x200 image, FPS and ODS </span><span class="font24" style="font-style:italic;">F-measure</span><span class="font24"> are evaluated on the BSDS500 test set.</span></p>
<table border="1">
<tr><td></td><td>
<p><span class="font17" style="font-style:italic;">HED</span><span class="font22"> </span><a href="#bookmark16"><span class="font22">[60]</span></a></p></td><td>
<p><span class="font17" style="font-style:italic;">RCF</span><span class="font22"> </span><a href="#bookmark18"><span class="font22">[31]</span></a></p></td><td>
<p><span class="font17" style="font-style:italic;">BDCN</span><span class="font22"> </span><a href="#bookmark30"><span class="font22">[18]</span></a></p></td><td>
<p><span class="font17" style="font-style:italic;">PiDiNet</span></p></td><td>
<p><span class="font17" style="font-style:italic;">PiDiNet(tiny)</span></p></td></tr>
<tr><td>
<p><span class="font22">Params</span></p></td><td>
<p><span class="font22">14.7M</span></p></td><td>
<p><span class="font22">14.8M</span></p></td><td>
<p><span class="font22">16.3M</span></p></td><td>
<p><span class="font22">710K</span></p></td><td>
<p><span class="font22">73K</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font22">MACs</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">22.2G</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">16.2G</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">23.2G</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">3.43G</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">270M</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font22">Throughput</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">78FPS</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">67FPS</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">47FPS</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">92FPS</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">215FPS</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font22">Pre-training</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">ImageNet</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">ImageNet</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">ImageNet</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">No</span></p></td><td style="vertical-align:bottom;">
<p><span class="font22">No</span></p></td></tr>
<tr><td>
<p><span class="font22">ODS </span><span class="font17" style="font-style:italic;">F-measure</span></p></td><td>
<p><span class="font22">0.788</span></p></td><td>
<p><span class="font22">0.806</span></p></td><td>
<p><span class="font22">0.820</span></p></td><td>
<p><span class="font22">0.807</span></p></td><td>
<p><span class="font22">0.787</span></p></td></tr>
</table>
<p><span class="font25">patch </span><a href="#bookmark13"><span class="font25">[2,</span></a><a href="#bookmark15"><span class="font25"> 47,</span></a><a href="#bookmark31"><span class="font25"> 3]</span></a><span class="font25">. Building on top of fully convolutional networks </span><a href="#bookmark32"><span class="font25">[33]</span></a><span class="font25">, HED </span><a href="#bookmark16"><span class="font25">[60]</span></a><span class="font25"> performs end-to-end edge detection by leveraging multilevel image features with rich hierarchical information guided by deep supervision, and achieves state-of-the-art performance. Other similar works include </span><a href="#bookmark33"><span class="font25">[62,</span></a><a href="#bookmark14"><span class="font25"> 23,</span></a><a href="#bookmark34"><span class="font25"> 36,</span></a><a href="#bookmark17"><span class="font25"> 55,</span></a><a href="#bookmark35"><span class="font25"> 61,</span></a><a href="#bookmark18"><span class="font25"> 31,</span></a><a href="#bookmark36"><span class="font25"> 8,</span></a><a href="#bookmark30"><span class="font25"> 18]</span></a><span class="font25">.</span></p>
<p><span class="font25">However, integration of traditional edge detectors with modern CNNs were rarely investigated. The former were merely utilized as auxiliary tools to extract candidate edge points in some prior approaches </span><a href="#bookmark31"><span class="font25">[3,</span></a><a href="#bookmark13"><span class="font25"> 2]</span></a><span class="font25">. Intuitively, edges manifest diverse specific patterns like straight lines, corners, and ''X” junctions. On one hand, traditional edge operators like those shown in Fig. 1 are inspired by these intuitions, and based on gradient computing which encodes important gradient information for edge detection by explicitly calculating pixel differences. However, these handcrafted edge operators or learning based edge detection algorithms are usually not powerful enough due to their shallow structures. On the other hand, modern CNNs can learn rich and hierarchical image representations, where vanilla CNN kernels serve as probing local image patterns. Nevertheless, CNN kernels are optimized by starting from random initialization which has no explicit encoding for gradient information, making them hard to focus on edge related features.</span></p>
<p><span class="font25">We believe a new type of convolutional operation can be derived, to satisfy the following needs. Firstly, it can easily capture the image gradient information that facilitates edge detection, and the CNN model can be more focused with the release of burden on dealing with much unrelated image features. Secondly, the powerful learning ability of deep CNNs can still be preserved, to extract semantically meaningful representations, which lead to robust and accurate edge detection. In this paper, we propose pixel difference convolution (PDC), where the pixel differences in the image are firstly computed, and then convolved with the kernel weights to generate output features (see Fig.</span><a href="#bookmark37"><span class="font25"> 3)</span></a><span class="font25">. We show PDC can effectively improve the quality of the output edge maps, as illustrated in Fig.</span><a href="#bookmark28"><span class="font25"> 2.</span></a></p>
<p><span class="font25">On the other hand, leading CNN based edge detectors suffer from the deficiencies as shown in Table</span><a href="#bookmark29"><span class="font25"> 1:</span></a><span class="font25"> being memory consuming with big model size, being energy hungry with high computational cost, running inefficiency with low throughput and label inefficiency with the need of model pre-training on large scale dataset. This is due to the fact that the annotated data available for training edge detection models is limited, and thus a well pretrained (usually large) backbone is needed. For example, the widely adopted routine is to use the large VGG16 </span><a href="#bookmark38"><span class="font25">[49]</span></a><span class="font25"> architecture that was trained on the large scale ImageNet dataset </span><a href="#bookmark39"><span class="font25">[7]</span></a><span class="font25">.</span></p>
<p><span class="font25">It is important to develop a lightweight structure, to achieve a better trade-off between accuracy and efficiency for edge detection. With pixel difference convolution, inspired by </span><a href="#bookmark40"><span class="font25">[19,</span></a><a href="#bookmark41"><span class="font25"> 20]</span></a><span class="font25">, we build a new end-to-end architecture, namely Pixel Difference Network (PiDiNet) to solve the mentioned issues in one time. Specifically, PiDiNet consists of an efficient backbone and an efficient task-specific side structure (see Fig.</span><a href="#bookmark42"><span class="font25"> 5)</span></a><span class="font25">, able to do robust and accurate edge detection with high efficiency.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark43"></a><span class="font26" style="font-weight:bold;">2. Related Work</span></h3></li></ul>
<p><span class="font25" style="font-weight:bold;">Using Traditional Edge Detectors to Help Deep CNN Models for Edge Detection. </span><span class="font25">Canny [ ] and SE ['] edge detectors are usually used to extract candidate contour points before applying the CNN model for contour/non-contour prediction [2,: ]. The candidate points can be also used as auxiliary relaxed labels for better training the CNN model [3</span><span class="font21">：</span><span class="font25"> ]. Instead of relying on the edge information from the hand-crafted detectors, PDC directly integrates the gradient information extraction process into the convolutional operation, which is more compact and learnable.</span></p>
<p><span class="font25" style="font-weight:bold;">Lightweight Architectures for Edge Detection. </span><span class="font25">Recently, efforts have been made to design lightweight architectures for efficient edge detection [56, 57, 45]. Some of them may not need a pretrained network based on large scale dataset [45]. Although being compact and fast, the detection accuracies with these networks are unsatisfactory. Alternatively, lightweight architectures for other dense prediction tasks [ </span><span class="font25" style="font-style:italic;">[3,</span><span class="font25"> 58, 43, 25, 38, 63] and multi-task learning [24, 26] may also benefit edge detection. However, the introduced sophisticated multi-branch based structures may lead to running inefficiency. Instead, we build a backbone structure which only uses a simple shortcut [ ] as the second branch for the convolutional blocks.</span></p>
<p><span class="font25" style="font-weight:bold;">Integrating Traditional Operators. </span><span class="font25">The proposed PDC is mostly related to the recent central difference convolution (CDC) [66, 65, </span><span class="font25" style="font-style:italic;">64,</span><span class="font25"> 67] and local binary convolution (LBC) [2 ], of which both derive from local binary patterns (LBP) [4</span><span class="font21">：</span><span class="font25"> ] and involve calculating pixel differences during convolution. LBC uses a set of predefined sparse binary filters to generalize the traditional LBP, focusing on reducing the network complexity. CDC further proposes to use learnable weights to capture image gradient information for robust face anti-spoofing. CDC can be seen as one instantiated case of the proposed PDC </span><span class="font25" style="font-style:italic;">(L e.,</span><span class="font25"> Central PDC), where the central direction is considered, as we will introduce in Section 3. Like CDC, PDC uses learnable filters while being more general and flexible to capture rich gradient information for edge detection. On the other hand, Gabor convolution [3- ] encodes the orientation and scale information in the convolution kernels by multiplying the kernels with a group of Gabor filters, while PDC is more compact without any auxiliary traditional feature filters.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark44"></a><span class="font26" style="font-weight:bold;">3. Pixel Difference Convolution</span></h3></li></ul>
<p><span class="font25">The process of pixel difference convolution (PDC) is pretty similar to that of vanilla convolution, where the original pixels in the local feature map patch covered by the convolution kernels are replaced by pixel differences, when conducting the convolutional operation. The formulations</span></p>
<div><img src="pidinet_files/pidinet-11.jpg" alt="" style="width:236pt;height:141pt;">
<p><span class="font24">Figure 3. Three instances of pixel difference convolution derived from extended LBP descriptors [28, 30, 52]. One can derive other instances by designing the picking strategy of the pixel pairs.</span></p>
</div><br clear="all">
<p><span class="font25">of vanilla convolution and PDC can be written as:</span></p>
<p><span class="font17" style="font-style:italic;">kxk</span></p>
<p><span class="font17" style="font-style:italic;">y = &nbsp;&nbsp;&nbsp;&nbsp;= y^<sup>w</sup>i</span><span class="font17"> •旳,</span></p>
<p><span class="font17" style="font-style:italic;">i=l</span></p>
<div style="border-bottom:solid;">
<p><span class="font25">(vanilla convolution)</span></p>
</div><br clear="all">
<p><span class="font26" style="font-style:italic;">y = &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font17" style="font-weight:bold;">= £ 凹•</span><span class="font21">(</span><span class="font25">X -</span><span class="font17" style="font-weight:bold;">葛)，</span><span class="font21">(</span><span class="font25">PDC)</span></p>
<p><span class="font25">where, </span><span class="font25" style="font-style:italic;">Xi</span><span class="font25"> and </span><span class="font25" style="font-style:italic;">x?</span><span class="font25"> are the input pixels, </span><span class="font25" style="font-style:italic;">Wi</span><span class="font25"> is the weight in the # x # convolution kernel. &nbsp;&nbsp;&nbsp;</span><span class="font25" style="font-style:italic;">V =</span></p>
<p><span class="font10" style="font-variant:small-caps;">{(⑦l</span><span class="font17"> 是)，(⑦</span><span class="font25">2,</span><span class="font17">时)，...，(⑦⑦人)} </span><span class="font25">is the set of pixel pairs picked from the current local patch, and </span><span class="font25" style="font-style:italic;">m &lt;&nbsp;k x k.</span></p>
<p><span class="font25">To capture rich gradient information, the pixel pairs can be selected according to different strategies, which can be inspired from the numerous traditional feature descriptors. Here, we utilize the ideas from the work in [42, 30, 52], where the local binary pattern (LBP) and its robust variants, extended LBP (ELBP), were used to encode pixel relations from varying directions (angular and radial). Specifically, ELBP are obtained by firstly calculating the pixel differences within a local patch (from </span><span class="font25" style="font-style:italic;">m</span><span class="font25"> pixel pairs), resulting in a pixel difference vector, and then binarizing the vector to create an m-length 0/1 code. Then, the bag-of-words technique [27] is usually used to calculate the code distribution (or histogram), which is regarded as the image representation. In ELBP, the angular and radial directions were will demonstrated to help encode potential discriminative image cues and be complementary for increasing the feature representational capacity for various computer vision tasks, such as texture classification [30, 2$ ] and face recognition [5 ].</span></p>
<p><span class="font25">By integrating ELBP with CNN convolution, we derive three types of PDC instances as shown in Fig. 3, in which we name them as central PDC (CPDC), angular PDC (APDC) and radial PDC (RPDC) respectively. The pixel pairs in the local patch is easy to understand. For example, for the APDC with kernel size 3 x 3, we create 8 pairs in the angular direction in the 3 x 3 local patch (thus </span><span class="font25" style="font-style:italic;">m</span><span class="font25"> = 8), then the pixel differences obtained from the pairs are convolved with the kernel by doing an element-wise multiplication with the kernel weights, followed by a summation, to generate the value in the output feature map.</span></p>
<p><span class="font25">The derived PDC instances based on ELBP can be seen as an extension of ELBP that are more flexible and learnable. Although being powerful, the original ELBP codes are discrete with limited representative ability. While the useful encodings of pixel relations in PDC will be preserved in the trained convolution kernels, as during the training process of CNN, the convolution kernels will be encouraged to have higher inner product with those important encodings, in order to create higher activation responses<a name="footnote1"></a><sup><a href="#bookmark45">1</a></sup><sup></sup>. By training from abundant of data, PDC is able to automatically learn rich representative encodings for the task.</span></p><img src="pidinet_files/pidinet-12.jpg" alt="" style="width:214pt;height:68pt;">
<p><span class="font25" style="font-weight:bold;">Converting PDC to Vanilla Convolution. </span><span class="font25">According to Eq. 6, one may notice that the computational cost and memory footprint by PDC are doubled compared with the vanilla counterpart. However, once the convolution kernels have been learnt, PDC layers can be converted to vanilla convolutional layers by instead saving the differences of the kernel weights in the model, according to the locations of the selected pixel pairs. In this way, the efficiency is maintained during inference. Taking APDC as an example (Fig. 11), conversion is done with the following equations:</span></p>
<p><span class="font8">g = &nbsp;&nbsp;</span><span class="font17">.(力</span><span class="font8">1 </span><span class="font17">一 </span><span class="font17" style="font-style:italic;">X2)</span><span class="font8">+ W<sub>2</sub> -</span><span class="font17" style="font-style:italic;">(X2 -</span><span class="font17">力</span><span class="font8">3)+ W<sub>3</sub> </span><span class="font17">•(力</span><span class="font8">3 -</span><span class="font17">力</span><span class="font8">6)+ …</span></p>
<p><span class="font8">=(W1 — W4)• </span><span class="font17" style="font-style:italic;">Xi +(W2 — W1) • X2</span><span class="font8"> +(W3 —</span><span class="font17">迎</span><span class="font8">2)-</span><span class="font17">力</span><span class="font8">3 + •••</span></p>
<p><span class="font17">=而</span><span class="font8">1 </span><span class="font17">•力</span><span class="font8">1 </span><span class="font17">+ 而</span><span class="font8">2 </span><span class="font17">•力</span><span class="font8">2 </span><span class="font17">+ 而</span><span class="font8">3 </span><span class="font17">•力</span><span class="font8">3 + ... = £ </span><span class="font8" style="font-style:italic;">饥</span><span class="font17">，<sup>Xi</sup>- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;°)</span></p>
<p><span class="font25">It is worth mentioning that we can also use this tweak to speed up the training process, where the differences of kernel weights are firstly calculated, followed by the convolution with the untouched input feature maps. We have illustrated more details in the appendix.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark46"></a><span class="font26" style="font-weight:bold;">4. PiDiNet Architecture</span></h3></li></ul>
<p><span class="font25">As tried by some prior works [56,45, 57], we believe it is both necessary and feasible to solve the inefficiency issues mentioned in Section 1 in one time by building an architecture with small model size and high running efficiency, and can be trained from scratch using limited datasets for effective edge detection. We construct our architecture with the following parts (Fig. 5).</span></p>
<p><span class="font25" style="font-weight:bold;">Efficient Backbone. </span><span class="font25">The building principle for the backbone is to make the structure slim while own high running efficiency. Thus we do not consider the sophisticated multi-branch lightweight structures proposed for many other tasks [13, 38, 6 ], since they may not appeal to parallel implementation [3: ], leading to unsatisfactory efficiency for the edge detection task. Inspired from [T ] and [2( ], we use the separable depth-wise convolutional structure with a shortcut for fast inference and easy training. The whole backbone has 4 stages and max pooling layers are among them for down sampling. Each stage has 4 residual blocks (except the first stage that has an initial convolutional layer and 3 residual blocks). The residual path in each block includes a dep th-wise convolutional layer, a ReLU layer, and a point-wise convolutional layer sequentially. The number of channels in each stage is reasonably small to avoid big model size </span><span class="font25" style="font-style:italic;">(C,</span><span class="font25"> 2 x C, 4 x C and 4 x (7 channels for stage 1, 2, 3, and 4 respectively).</span></p>
<p><span class="font25" style="font-weight:bold;">Efficient Side Structure. </span><span class="font25">To learn rich hierarchical edge representation, we also use the side structure as in [60] to generate an edge map from each stage respectively, based on which a side loss is computed with the ground truth map to provide deep supervision [60]. To refine the feature maps, beginning from the end of each stage, we firstly build a compact dilation convolution based module (CDCM) to enrich multi-scale edge information, which takes the input with </span><span class="font25" style="font-style:italic;">n x C</span><span class="font25"> channels, and produces </span><span class="font25" style="font-style:italic;">M (M &lt;&nbsp;C)</span><span class="font25"> channels in the output to relieve the computation overhead, followed by a compact spatial attention module (CSAM) to eliminate the background noise. After that, a 1 x 1 convolutional layer further reduces the feature volume to a single channel map, which is then interpolated to the original size followed by a Sigmoid function to create the edge map. The final edge map, which is used for testing, is created by fusing the 4 single channel feature maps with a concatenation, a convolutional layer and a Sigmoid function.</span></p>
<p><span class="font25">The detailed structure information can be seen in Fig. 5, noting that we do not use any normalization layers for simplicity since the resolutions of the training images are not uniform. The obtained architecture is our baseline. By replacing the vanilla convolution in the 3 x 3 depth-wise convolutional layer in the residual blocks with PDC, we get the proposed PiDiNet.</span></p>
<p><span class="font25" style="font-weight:bold;">Loss Function. </span><span class="font25">We adopt the annotator-robust loss function proposed in [31] for each generated edge map (including the final edge map). For the zth pixel in the </span><span class="font17">顶</span><span class="font25">th edge map with value </span><span class="font25" style="font-style:italic;">pj</span><span class="font25">, the loss is calculated as:</span></p>
<p><span class="font17">仲</span><span class="font7">・</span><span class="font24">log(l — p</span><span class="font20">；</span><span class="font24">) if </span><span class="font17">功</span><span class="font24">=0</span></p>
<p><span class="font24">H={o &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if0&lt;^&lt;?7 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4)</span></p>
<p><span class="font24" style="font-style:italic;">[0</span><span class="font24"> • log &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;otherwise,</span></p>
<p><span class="font25">where </span><span class="font17">切 </span><span class="font25">is the ground truth edge probability, 77 is a predefined threshold, meaning that a pixel is discarded and not</span></p>
<div>
<p><a name="bookmark47"></a><span class="font1">Image</span></p>
</div><br clear="all">
<div>
<p><span class="font1" style="text-decoration:underline;">. Sigmoid &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final EdgeMap</span><span class="font1"> &lt;—</span></p>
<p><span class="font12" style="text-decoration:underline;">. 个 </span><span class="font1" style="text-decoration:underline;">.</span><span class="font1">1 channel</span></p>
<p><span class="font1" style="text-decoration:underline;">'^Ixl-iconv |</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Ground Truth</span></p>
</div><br clear="all">
<div>
<p><span class="font1" style="font-weight:bold;">(A) Block_x_y</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Init conv &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Stage 1</span></p>
<p><span class="font1">C channels</span></p>
<p><span class="font1">Block 1 1</span></p>
<p><span class="font1">Block_1_2</span></p>
<p><span class="font1">Block_1_3 &nbsp;&nbsp;—&gt;</span></p>
</div><br clear="all">
<div>
<p><span class="font1">CDCM</span></p>
</div><br clear="all">
<div>
<p><span class="font1">M channels</span></p>
</div><br clear="all">
<div>
<p><span class="font1">CSAM</span></p>
</div><br clear="all">
<div>
<p><span class="font1">—► 1x1-1</span><span class="font1" style="text-decoration:underline;"> </span><span class="font1">Conv</span></p>
</div><br clear="all">
<div>
<p><span class="font1">4 channels Concatenate ]</span></p>
<p><span class="font5">I</span></p>
</div><br clear="all">
<div>
<p><span class="font1">1 channel</span></p>
</div><br clear="all">
<div>
<p><span class="font1">-* Sigmoid &nbsp;&nbsp;―►</span><span class="font1" style="text-decoration:underline;"> </span><span class="font1">EdgeMap</span><span class="font1" style="text-decoration:underline;"> </span><span class="font1">1</span><span class="font1" style="text-decoration:underline;"> &nbsp;</span><span class="font1">&lt;—</span></p>
</div><br clear="all">
<div>
<p><span class="font1">2x2 Pool</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Block_2_1</span></p>
</div><br clear="all">
<div>
<p><span class="font1" style="text-decoration:underline;">Block_2_2</span></p>
<p><span class="font1" style="text-decoration:underline;">Block_2_3</span></p>
<p><span class="font1">Block_2_4</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Stage 2</span></p>
<p><span class="font1">2xC channels</span></p>
</div><br clear="all">
<div>
<p><span class="font1">M channels</span></p>
</div><br clear="all">
<div>
<p><span class="font1">1 channel</span></p>
</div><br clear="all">
<div>
<p><span class="font1">CDCM</span></p>
</div><br clear="all">
<div>
<p><span class="font1">CSAM</span></p>
</div><br clear="all">
<div>
<p><span class="font1">1x1-1 Conv</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Interpolate</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Sigmoid</span></p>
</div><br clear="all">
<div>
<p><span class="font1">EdgeMap 2</span></p>
</div><br clear="all">
<div>
<p><span class="font1">nC x H x W</span></p><img src="pidinet_files/pidinet-13.jpg" alt="" style="width:59pt;height:91pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-14.jpg" alt="" style="width:51pt;height:104pt;">
</div><br clear="all">
<div>
<p><span class="font1">2x2 Pool</span></p>
</div><br clear="all">
<div>
<p><a href="#bookmark48"><span class="font1" style="text-decoration:underline;">| &nbsp;&nbsp;&nbsp;Block 3 1</span></a></p>
<p><a href="#bookmark49"><span class="font1" style="text-decoration:underline;">| &nbsp;&nbsp;&nbsp;Block 3 2</span></a></p>
<p><a href="#bookmark50"><span class="font1">| &nbsp;&nbsp;&nbsp;Block 3 3</span></a></p>
</div><br clear="all">
<div>
<p><span class="font1">Stage 3 4xC channels</span></p>
</div><br clear="all">
<div>
<p><span class="font1" style="text-decoration:underline;">| &nbsp;&nbsp;Block 3 4 &nbsp;&nbsp;|</span><span class="font1">-&gt;f</span></p>
</div><br clear="all">
<div>
<p><span class="font1">CDCM</span></p>
</div><br clear="all">
<div>
<p><span class="font1">M channels</span></p>
</div><br clear="all">
<div>
<p><span class="font1">1 channel</span></p>
</div><br clear="all">
<div>
<p><span class="font1">2x2 Pool</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Block 4 1</span></p>
<p><span class="font1">Block 4 2</span></p>
<p><span class="font1">Block 4 3</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Stage 4 4xC channels</span></p>
</div><br clear="all">
<div>
<p><span class="font1">CSAM</span></p>
</div><br clear="all">
<div>
<p><span class="font1">M channels</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Block 4 4 &nbsp;&nbsp;—► &nbsp;&nbsp;CDCM</span><span class="font1" style="text-decoration:underline;"> &nbsp;&nbsp;&nbsp;</span><span class="font1">—► &nbsp;&nbsp;&nbsp;CSAM</span></p>
</div><br clear="all">
<div>
<p><span class="font3" style="font-variant:small-caps;">~a</span><span class="font3" style="font-variant:small-caps;text-decoration:underline;">|</span><span class="font1" style="text-decoration:underline;"> 1x1-1 Conv J</span><span class="font1">—»</span><span class="font1" style="text-decoration:underline;">| &nbsp;Interpolate &nbsp;|</span><span class="font1">—</span><span class="font1" style="text-decoration:underline;">H &nbsp;&nbsp;Sigmoid &nbsp;&nbsp;</span><span class="font3" style="font-variant:small-caps;text-decoration:underline;">)</span><span class="font3" style="font-variant:small-caps;">~a</span><span class="font3" style="font-variant:small-caps;text-decoration:underline;">|</span><span class="font1" style="text-decoration:underline;"> EdgeMap 3 |</span><span class="font1">«—</span></p>
</div><br clear="all">
<div>
<p><span class="font1">1 channel</span></p>
</div><br clear="all">
<div>
<p><span class="font1">—*</span><span class="font1" style="text-decoration:underline;"> </span><span class="font1">1x1-1</span><span class="font1" style="text-decoration:underline;"> </span><span class="font1">Conv &gt;</span><span class="font1" style="text-decoration:underline;">&nbsp;</span><span class="font1">Interpolate</span><span class="font1" style="text-decoration:underline;"> &nbsp;</span><span class="font1">—►</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Sigmoid</span><span class="font1" style="text-decoration:underline;"> &nbsp;&nbsp;</span><span class="font1">―&gt;</span><span class="font1" style="text-decoration:underline;"> </span><span class="font1">EdgeMap</span><span class="font1" style="text-decoration:underline;"> </span><span class="font1">4</span><span class="font1" style="text-decoration:underline;"> </span><span class="font1">&lt;~</span></p><img src="pidinet_files/pidinet-15.jpg" alt="" style="width:134pt;height:88pt;">
</div><br clear="all">
<p><a name="bookmark42"></a><span class="font24">Figure 5. PiDiNet architecture.</span></p>
<p><span class="font25">considered to be a sample when calculating the loss if it is marked as positive by fewer than n of annotators to avoid confusing,</span><span class="font17">月 </span><span class="font25">is the percentage of negative pixel samples and a =</span><span class="font17">入</span><span class="font25">. (1 -</span><span class="font17">月</span><span class="font25">).After all, the total loss is </span><span class="font25" style="font-style:italic;">L</span><span class="font25"> = </span><span class="font13" style="font-style:italic;">£七 </span><span class="font25" style="font-style:italic;">j l{</span><span class="font25">.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark51"></a><span class="font26" style="font-weight:bold;">5. Experiments</span></h3>
<ul style="list-style:none;">
<li>
<h4><a name="bookmark52"></a><span class="font25" style="font-weight:bold;">5.1. Datasets and Implementation</span></h4></li></ul></li></ul>
<p><span class="font25" style="font-weight:bold;">Experimental Datasets. </span><span class="font25">We evaluate the proposed PiDiNet on three widely used datasets, namely, BSDS500 </span><a href="#bookmark53"><span class="font25">[1]</span></a><span class="font25">, NYUD </span><a href="#bookmark54"><span class="font25">[48]</span></a><span class="font25">, and Multicue </span><a href="#bookmark55"><span class="font25">[39]</span></a><span class="font25">. The experimental settings about data augmentation and configuration on the three datasets follow </span><a href="#bookmark16"><span class="font25">[60,</span></a><a href="#bookmark18"><span class="font25"> 31,</span></a><a href="#bookmark30"><span class="font25"> 18]</span></a><span class="font25"> and the details are given below. BSDS500 consists of 200, 100, and 200 images in the training set, validation set, and test set respectively. Each image has 4 to 9 annotators. Training images in the dataset are augmented with flipping (2x), scaling (3x), and rotation (16x), leading to a training set that is 96x larger than the unaugmented version. Like prior works </span><a href="#bookmark16"><span class="font25">[60,</span></a><a href="#bookmark18"><span class="font25"> 31,</span></a><a href="#bookmark30"><span class="font25"> 18]</span></a><span class="font25">, the PASCAL VOC Contextdataset</span><a href="#bookmark56"><span class="font25">[40]</span></a><span class="font25">, which has 10K labeled images (and augmented to 20K with flipping), is also optionally considered in training. NYUD has 1449 pairs of aligned RGB and depth images which are densely labeled. There are 381, 414 and 654 images for training, validation, and test respectively. We combine the training and validation set and augment them with flipping (2x), scaling (3x), and rotation (4x) to produce the training data. Multicue is composed of 100 challenging natural scenes and each scene contains a left- and rightview color sequences captured by a binocular stereo camera. The last frame of left-view sequences for each scene, which is labeled with edges and boundaries, is used in our experiments. We randomly split them to 80 and 20 images for training and evaluation respectively. The process is independently repeated twice more. The metrics are then recorded from the three runs. We also augment each training image with flipping (2x), scaling (3x), and rotation (16x), then randomly crop them with size 500x500.</span></p>
<p><span class="font25" style="font-weight:bold;">Performance Metrics. </span><span class="font25">During evaluation, </span><span class="font25" style="font-style:italic;">F-measure </span><span class="font25">at both Optimal Dataset Scale (ODS) and Optimal Image Scale (OIS) are recorded for all datasets. Since efficiency is one of the main focuses in this paper, all the models are compared based on the evaluations from single scale images if not specified.</span></p>
<p><span class="font25" style="font-weight:bold;">Implementation Details. </span><span class="font25">Our implementation is based on the Pytorch library </span><a href="#bookmark57"><span class="font25">[44]</span></a><span class="font25">. In detail, PiDiNet (and the baseline) is randomly initialized and trained for 14 epochs with Adam optimizer </span><a href="#bookmark58"><span class="font25">[22]</span></a><span class="font25"> with an initial learning rate 0.005, which is decayed in a multi-step way (at epoch 8 and 12 with decaying rate 0.1). If VOC dataset is used in training for evaluating BSDS500, we train 20 epochs and decay the learning rate at epoch 10 and 16.</span><span class="font17">入 </span><span class="font25">is set to 1.1 for both BSDS500 and Multicue, and 1.3 for NYUD. The threshold n is set to 0.3 for both BSDS500 and Multicue. No n is needed for NYUD since the images are singly annotated.</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark59"></a><span class="font25" style="font-weight:bold;">5.2. Ablation Study</span></h4></li></ul>
<p><span class="font25">To demonstrate the effectiveness of PDC and to find the possibly optimal architecture configuration, we conduct our ablation study on the BSDS500 dataset, where we use the data augmented from the 200 images in the training set (optionally mixed with the VOC dataset) for training and record the metrics on the validation set.</span></p>
<p><span class="font25" style="font-weight:bold;">Architecture Configuration. </span><span class="font25">We can replace the vanilla convolution with PDC in any block (we also regard the ini-</span></p>
<p><a name="bookmark60"></a><span class="font24">Table 3. More comparisons between PiDiNet and the baseline architecture in multiple network scales by changing the nubmer of channels </span><span class="font17" style="font-style:italic;">C</span><span class="font24"> (see Fig.</span><a href="#bookmark42"><span class="font24"> 5)</span></a><span class="font24">. The models are trained using the BSDS500 training set, and evaluated on BSDS500 validation set.</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font23">Scale</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">Baseline (ODS / OIS)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">PiDiNet (ODS / OIS)</span></p></td></tr>
<tr><td>
<p><span class="font23">Tiny (C=20)</span></p></td><td>
<p><span class="font23">0.735/0.752</span></p></td><td>
<p><span class="font23" style="font-weight:bold;">0.747/0.764</span></p></td></tr>
<tr><td>
<p><span class="font23">Small (C=30)</span></p></td><td>
<p><span class="font23">0.738/0.759</span></p></td><td>
<p><span class="font23" style="font-weight:bold;">0.752/0.769</span></p></td></tr>
<tr><td>
<p><span class="font23">Normal (C=60)</span></p></td><td>
<p><span class="font23">0.736/0.751</span></p></td><td>
<p><span class="font23" style="font-weight:bold;">0.757/0.776</span></p></td></tr>
</table>
<p><a name="bookmark61"></a><span class="font24">Table 4. Ablation on CDCM, CSAM and shortcuts. The models are trained with BSDS500 training set and VOC dataset, and evaluated on BSDS500 validation set.</span></p>
<table border="1">
<tr><td>
<p><span class="font23">CSAM</span></p></td><td>
<p><span class="font23">CDCM</span></p></td><td>
<p><span class="font23">Shortcuts</span></p></td><td>
<p><span class="font23">ODS / OIS</span></p></td></tr>
<tr><td>
<p><span class="font2" style="font-weight:bold;">X</span></p></td><td>
<p><span class="font2" style="font-weight:bold;">X</span></p></td><td>
<p><span class="font13" style="font-style:italic;">/</span></p></td><td>
<p><span class="font23">0.770/0.790</span></p></td></tr>
<tr><td>
<p><span class="font2" style="font-weight:bold;">X</span></p></td><td>
<p><span class="font13" style="font-style:italic;">/</span></p></td><td>
<p><span class="font13" style="font-style:italic;">/</span></p></td><td>
<p><span class="font23">0.775/0.793</span></p></td></tr>
<tr><td>
<p><span class="font13" style="font-style:italic;">/</span></p></td><td>
<p><span class="font13" style="font-style:italic;">/</span></p></td><td>
<p><span class="font13" style="font-style:italic;">/</span></p></td><td>
<p><span class="font23" style="font-weight:bold;">0.776/0.795</span></p></td></tr>
<tr><td>
<p><span class="font13" style="font-style:italic;">/</span></p></td><td>
<p><span class="font13" style="font-style:italic;">/</span></p></td><td>
<p><span class="font2" style="font-weight:bold;">X</span></p></td><td>
<p><span class="font23">0.734/0.755</span></p></td></tr>
</table>
<div>
<p><a name="bookmark62"></a><span class="font24">Table 2. Possible configurations of PiDiNet. C, A, R and ‘V’ indicate CPDC, APDC, RPDC and vanilla convolution respectively. ‘xn’ means repeating the pattern for </span><span class="font17" style="font-style:italic;">n</span><span class="font24"> times sequentially. For example, the baseline architectrue can be presented as “[V] x 16”, and ‘C-[V] x 15’ means using CPDC in the first block and vanilla convolutions in the later blocks. All the models are trained using BSDS500 training set and the VOC dataset, then evaluated on BSDS500 validation set.</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font23">Architecture</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">C-[V]</span><span class="font2">x</span><span class="font23">15</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">A-[V]</span><span class="font2">x</span><span class="font23">15</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">R-[V]</span><span class="font2">x</span><span class="font23">15</span></p></td></tr>
<tr><td>
<p><span class="font23">ODS / OIS</span></p></td><td>
<p><span class="font23">0.775 / 0.794</span></p></td><td>
<p><span class="font23">0.774 / 0.794</span></p></td><td>
<p><span class="font23">0.774 / 0.792</span></p></td></tr>
<tr><td>
<p><span class="font23">Architecture</span></p></td><td>
<p><span class="font23">[CVVV]</span><span class="font2">x</span><span class="font23">4</span></p></td><td>
<p><span class="font23">[AVVV]</span><span class="font2">x</span><span class="font23">4</span></p></td><td>
<p><span class="font23">[RVVV]</span><span class="font2">x</span><span class="font23">4</span></p></td></tr>
<tr><td>
<p><span class="font23">ODS / OIS</span></p></td><td>
<p><span class="font23">0.773 / 0.792</span></p></td><td>
<p><span class="font23">0.771 /0.790</span></p></td><td>
<p><span class="font23">0.772/0.791</span></p></td></tr>
<tr><td>
<p><span class="font23">Architecture</span></p></td><td>
<p><span class="font23">[CCCV]</span><span class="font2">x</span><span class="font23">4</span></p></td><td>
<p><span class="font23">[AAAV]</span><span class="font2">x</span><span class="font23">4</span></p></td><td>
<p><span class="font23">[RRRV]</span><span class="font2">x</span><span class="font23">4</span></p></td></tr>
<tr><td>
<p><span class="font23">ODS / OIS</span></p></td><td>
<p><span class="font23">0.772/0.791</span></p></td><td>
<p><span class="font23">0.775 / 0.793</span></p></td><td>
<p><span class="font23">0.771 /0.787</span></p></td></tr>
<tr><td>
<p><span class="font23">Architecture</span></p></td><td>
<p><span class="font23">[C]</span><span class="font2">x</span><span class="font23">16</span></p></td><td>
<p><span class="font23">[A]</span><span class="font2">x</span><span class="font23">16</span></p></td><td>
<p><span class="font23">[R]</span><span class="font2">x</span><span class="font23">16</span></p></td></tr>
<tr><td>
<p><span class="font23">ODS / OIS</span></p></td><td>
<p><span class="font23">0.767 / 0.786</span></p></td><td>
<p><span class="font23">0.768 / 0.786</span></p></td><td>
<p><span class="font23">0.758 / 0.777</span></p></td></tr>
<tr><td>
<p><span class="font23">Architecture</span></p></td><td>
<p><span class="font23">Baseline</span></p></td><td colspan="2">
<p><span class="font23" style="font-weight:bold;">[CARV]</span><span class="font2">x</span><span class="font23" style="font-weight:bold;">4 (PiDiNet)</span></p></td></tr>
<tr><td>
<p><span class="font23">ODS / OIS</span></p></td><td>
<p><span class="font23">0.772 / 0.792</span></p></td><td colspan="2">
<p><span class="font23" style="font-weight:bold;">0.776/0.795</span></p></td></tr>
</table>
</div><br clear="all">
<p><span class="font25">tial convolutional layer as a block in the context) in the backbone. Since there are 16 blocks, and a brute force search for the architecture configurations is not feasible, hence we only sample some of them as shown in Table</span><a href="#bookmark62"><span class="font25"> 2 </span></a><span class="font25">by gradually increasing the number of PDCs. We found replacing the vanilla convolution with PDC only in a single block can even have obvious improvement. More replacements with the same type of PDC may no longer give extra performance gain and instead degenerate the model. We conjecture that the PDC in the first block already obtains much gradient information from the raw image, and an abuse of PDC may even cause the model fail to preserve useful in-</span></p>
<div>
<p><span class="font0">■ BSDS500</span></p>
<p><span class="font0">♦ BSDS500 + VOC</span></p>
<p><span class="font16">10 9 8 .<sup>8</sup> .<sup>8</sup> .<sup>7</sup> .<sup>7 </sup>o o o o </span><span class="font15" style="font-style:italic;">a)</span><span class="font14" style="font-style:italic;">」</span><span class="font15" style="font-style:italic;">snB ① wULg</span><span class="font14" style="font-style:italic;">。</span></p><img src="pidinet_files/pidinet-16.jpg" alt="" style="width:210pt;height:54pt;">
</div><br clear="all">
<div>
<p><span class="font1">0.82</span></p>
<p><span class="font1">0.81</span></p>
<p><span class="font1">0.80</span></p><img src="pidinet_files/pidinet-17.jpg" alt="" style="width:212pt;height:58pt;">
<p><span class="font1">Normal (C=60) 710K</span></p>
<p><span class="font1">Wide (C=90) 1.58M</span></p>
<p><span class="font1">Wider (C=120) 2.79M</span></p>
<p><a name="bookmark63"></a><span class="font24">Figure 6. Exploration on the scalability of PiDiNet. The structure sizes are changed by slimming or widening the basic PiDiNet. Bottom row shows the number of parameters for each model. The models are trained with or without VOC dataset.</span></p>
<p><span class="font1">Tiny &nbsp;&nbsp;&nbsp;Samll</span></p>
<p><span class="font1">(C=20) (C=30) 84K &nbsp;&nbsp;&nbsp;184K</span></p>
</div><br clear="all">
<div>
<p><span class="font1">0.79 L.</span></p>
</div><br clear="all">
<p><span class="font25">formation. The extreme case is that when all the blocks are configured with PDC, the performance becomes worse than that of the baseline. The best configuration is ‘[CARV] x4’, which means combing the 4 types of convolutions sequentially in each stage, as different types of PDC capture the gradient information in different encoding directions. We will use this configuration in the following experiments.</span></p>
<p><span class="font25">To further demonstrate the superiority of PiDiNet over the baseline, which only uses the vanilla convolution, we give more comparisons as shown in Table</span><a href="#bookmark60"><span class="font25"> 3.</span></a><span class="font25"> It constantly proves that PDC configured architectures outperform the corresponding vanilla convolution configured architectures.</span></p>
<p><span class="font25" style="font-weight:bold;">CSAM, CDCM and Shortcuts. </span><span class="font25">The effectiveness of CSAM, CDCM and residual structures are demonstrated in Table</span><a href="#bookmark61"><span class="font25"> 4.</span></a><span class="font25"> The addition of shortcuts is simple yet important, as they can help preserve the gradient information captured by the previous layers. On the other hand, the attention mechanism in CSAM and dilation convolution in CDCM can give extra performance gains, while may also bring some computational cost. Therefore, they can be used to tradeoff between accuracy and efficiency. In the following experiments, we note PiDiNet without CSAM and CDCM as PiDiNet-L (meaning a more lightweight version).</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark64"></a><span class="font25" style="font-weight:bold;">5.3. Network Scalability</span></h4></li></ul>
<p><span class="font25">PiDiNet is highly compact with only 710K parameters and support training from scratch with limited training data. Here, we explore the scalability of PiDiNet with different model complexities as shown in Fig.</span><a href="#bookmark63"><span class="font25"> 6.</span></a><span class="font25"> In order to compare with other approaches, the models are trained in two schemes, both use the BSDS500 training and validation set, while with or without mixing the VOC dataset during training. Metrics are recorded on BSDS500 test set. As expected, compared with the basic PiDiNet, smaller models suffer from lower network capacity and thus with degenerated performances in terms of both ODS and OIS scores. At</span></p>
<div>
<p><a name="bookmark65"></a><span class="font1" style="font-weight:bold;">PiDiNet-tiny-L PiDiNet-small-L PiDiNet-tiny PiDiNet-small PiDiNet-L </span><span class="font1">FINED-Inf FINED-Train </span><span class="font1" style="font-weight:bold;">PiDiNet </span><span class="font1">BDCN2 HED BDCN3 RCF BDCN4 BDCN5</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-18.jpg" alt="" style="width:193pt;height:72pt;">
<p><span class="font1" style="font-style:italic;">70 FPS</span><span class="font1"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5</span></p>
<p><span class="font1" style="font-style:italic;">67 FPS</span></p>
<p><span class="font1" style="font-style:italic;">58 FPS</span><span class="font1"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ours &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font4"><sub>0</sub></span></p>
<p><span class="font1" style="font-style:italic;">47 FPS</span><span class="font1"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Others</span></p>
</div><br clear="all">
<div>
<p><span class="font1" style="font-style:italic;">0.807</span></p>
<p><span class="font1" style="font-style:italic;">0.800 0.7980.793 0.789 0.787</span></p>
<p><span class="font16" style="font-style:italic;">。华</span><span class="font1" style="font-style:italic;">.7=8 0.</span><span class="font18" style="font-style:italic;">；</span><span class="font1" style="font-style:italic;">880</span><span class="font18" style="font-style:italic;">；</span><span class="font1" style="font-style:italic;">90<sup>0</sup> </span><span class="font16" style="font-style:italic;">財.</span><span class="font1" style="font-style:italic;">796</span></p>
<table border="1">
<tr><td>
<p><span class="font1" style="font-style:italic;">…0.757 ”.....</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1" style="font-style:italic;">28.8M</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1" style="font-style:italic;">20M</span></p></td><td style="vertical-align:middle;">
<p><span class="font1" style="font-style:italic;">U14.7M</span></p></td></tr>
<tr><td></td><td></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1" style="font-style:italic;"><sup>71</sup>0K 611K184K159K 84K 73K 380K 485K</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1" style="font-style:italic;">| 1.08M <sup>143M</sup></span></p></td></tr>
</table>
<p><span class="font1" style="font-style:italic;">22M</span></p>
</div><br clear="all">
<div>
<p><span class="font1" style="font-style:italic;">0.806 0.808 0.812 <sup>0 820</sup></span></p>
<p><span class="font1">, &nbsp;&nbsp;. &nbsp;&nbsp;♦ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.803</span></p>
<p><span class="font1">Human</span></p>
</div><br clear="all">
<div>
<p><span class="font1" style="font-style:italic;">0.798</span></p>
</div><br clear="all">
<div>
<p><span class="font1">0.75</span></p>
</div><br clear="all">
<div>
<p><span class="font1" style="font-style:italic;">14.8M <sup>15 7M 16</sup></span><span class="font16" style="font-style:italic;">枷</span><span class="font1"> 0.7</span></p>
<p><span class="font1" style="font-style:italic;">8.69M</span></p><a name="caption2"></a>
<h1><a name="bookmark66"></a><span class="font6">1111</span></h1>
</div><br clear="all">
<div>
<p><span class="font1" style="font-style:italic;">2.26M</span></p><img src="pidinet_files/pidinet-19.jpg" alt="" style="width:233pt;height:28pt;">
<p><a name="bookmark67"></a><span class="font24">Figure 7. Comparison with other methods in terms of network complexity, running efficiency and detection performance (on BSDS500 dataset). The running speeds of FINED </span><a href="#bookmark68"><span class="font24">[56]</span></a><span class="font24"> are cited from the original paper, and the rest are evaluated by our implementations</span></p>
</div><br clear="all">
<div style="border-top:solid;">
<p><a name="bookmark69"></a><span class="font24">Table 5. Comparison with other methods on BSDS500 dataset. </span><span class="font12">* </span><span class="font24">indicates the speeds with our implementations based on a NVIDIA RTX 2080 Ti GPU. </span><span class="font1"><sup>1</sup> </span><span class="font24">indicates the cited GPU speeds.</span></p>
<table border="1">
<tr><td>
<p><span class="font23">Method</span></p></td><td>
<p><span class="font23">ODS</span></p></td><td>
<p><span class="font23">OIS</span></p></td><td>
<p><span class="font23">FPS</span></p></td></tr>
<tr><td>
<p><span class="font23">Human</span></p></td><td>
<p><span class="font23">.803</span></p></td><td>
<p><span class="font23">.803</span></p></td><td></td></tr>
<tr><td>
<p><span class="font23">Canny </span><a href="#bookmark5"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.611</span></p></td><td>
<p><span class="font23">.676</span></p></td><td>
<p><span class="font23">28</span></p></td></tr>
<tr><td>
<p><span class="font23">Pb </span><a href="#bookmark25"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.672</span></p></td><td>
<p><span class="font23">.695</span></p></td><td>
<p><span class="font23">-</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">SCG</span><a href="#bookmark24"><span class="font23">[]</span></a></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.739</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.758</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">-</span></p></td></tr>
<tr><td>
<p><span class="font23">SE</span><a href="#bookmark23"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.743</span></p></td><td>
<p><span class="font23">.763</span></p></td><td>
<p><span class="font23">12.5</span></p></td></tr>
<tr><td>
<p><span class="font23">OEF</span><a href="#bookmark22"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.746</span></p></td><td>
<p><span class="font23">.770</span></p></td><td>
<p><span class="font23">2/3</span></p></td></tr>
<tr><td>
<p><span class="font23">DeepEdge </span><a href="#bookmark13"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.753</span></p></td><td>
<p><span class="font23">.772</span></p></td><td>
<p><span class="font23">1/1000</span><span class="font1">t</span></p></td></tr>
<tr><td>
<p><span class="font23">DeepContour </span><a href="#bookmark15"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.757</span></p></td><td>
<p><span class="font23">.776</span></p></td><td>
<p><span class="font23">1/30</span><span class="font1">t</span></p></td></tr>
<tr><td>
<p><span class="font23">HFL</span><a href="#bookmark31"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.767</span></p></td><td>
<p><span class="font23">.788</span></p></td><td>
<p><span class="font23">5/6</span><span class="font1">t</span></p></td></tr>
<tr><td>
<p><span class="font23">CEDN</span><a href="#bookmark33"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.788</span></p></td><td>
<p><span class="font23">.804</span></p></td><td>
<p><span class="font23">10</span><span class="font1">t</span></p></td></tr>
<tr><td>
<p><span class="font23">HED</span><a href="#bookmark16"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.788</span></p></td><td>
<p><span class="font23">.808</span></p></td><td>
<p><span class="font23">78</span><span class="font1">*</span></p></td></tr>
<tr><td>
<p><span class="font23">DeepBoundary </span><a href="#bookmark14"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.789</span></p></td><td>
<p><span class="font23">.811</span></p></td><td>
<p><span class="font23">-</span></p></td></tr>
<tr><td>
<p><span class="font23">COB </span><a href="#bookmark34"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.793</span></p></td><td>
<p><span class="font23">.820</span></p></td><td>
<p><span class="font23">-</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">CED </span><a href="#bookmark17"><span class="font23">[]</span></a></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.794</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.811</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">-</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">AMH-Net </span><a href="#bookmark35"><span class="font23">[]</span></a></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.798</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.829</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">-</span></p></td></tr>
<tr><td>
<p><span class="font23">RCF</span><a href="#bookmark18"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.806</span></p></td><td>
<p><span class="font23">.823</span></p></td><td>
<p><span class="font23">67</span><span class="font1">*</span></p></td></tr>
<tr><td>
<p><span class="font23">LPCB </span><a href="#bookmark36"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.808</span></p></td><td>
<p><span class="font23">.824</span></p></td><td>
<p><span class="font23">30</span><span class="font1">t</span></p></td></tr>
<tr><td>
<p><span class="font23">BDCN</span><a href="#bookmark30"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.820</span></p></td><td>
<p><span class="font23">.838</span></p></td><td>
<p><span class="font23">47</span><span class="font1">*</span></p></td></tr>
<tr><td>
<p><span class="font23">FINED-Inf </span><a href="#bookmark68"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.788</span></p></td><td>
<p><span class="font23">.804</span></p></td><td>
<p><span class="font23">124</span><span class="font1">t</span></p></td></tr>
<tr><td>
<p><span class="font23">FINED-Train </span><a href="#bookmark68"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.790</span></p></td><td>
<p><span class="font23">.808</span></p></td><td>
<p><span class="font23">99</span><span class="font1">t</span></p></td></tr>
<tr><td>
<p><span class="font23">Baseline</span></p></td><td>
<p><span class="font23">.798</span></p></td><td>
<p><span class="font23">.816</span></p></td><td>
<p><span class="font23">96</span><span class="font1">*</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">PiDiNet</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.807</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.823</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">92</span><span class="font1">*</span><span class="font8">，</span><span class="font1">*</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">PiDiNet-L</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.800</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.815</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">128</span><span class="font1">*</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">PiDiNet-Small</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.798</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.814</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">148</span><span class="font1">*</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">PiDiNet-Small-L</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.793</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.809</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">212</span><span class="font1">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-Tiny</span></p></td><td>
<p><span class="font23">.789</span></p></td><td>
<p><span class="font23">.806</span></p></td><td>
<p><span class="font23">152</span><span class="font1">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-Tiny-L</span></p></td><td>
<p><span class="font23">.787</span></p></td><td>
<p><span class="font23">.804</span></p></td><td>
<p><span class="font23">215</span><span class="font1">*</span></p></td></tr>
</table>
<p><span class="font22">“PiDiNet” is slightly slower than “Baseline” because RPDC is</span></p>
<p><span class="font22">a 5x5 convolution after conversion.</span></p>
</div><br clear="all">
<p><span class="font25">the same time, training with more data constantly leads to higher accuracy. It is noted that the normal scale PiDiNet, can achieve the ODS and OIS scores at the same level as that recorded in the HED approach </span><a href="#bookmark16"><span class="font25">[60]</span></a><span class="font25">, even when trained from scratch only using the BSDS500 dataset </span><span class="font25" style="font-style:italic;">(i.e.,</span><span class="font25"> 0.789 vs. 0.788 in ODS and 0.803 vs. 0.808 in OIS for PiDiNet vs. HED). However, with limited training data, widening the PiDiNet architecture may cause the overfitting problem,</span></p>
<p><span class="font2">i</span></p>
<p><span class="font2">0.9</span></p>
<p><span class="font2">0.8</span></p>
<p><span class="font2">0.7</span></p>
<p><span class="font2">0.6</span></p>
<p><span class="font2">I 0.5</span></p>
<p><span class="font13">巴</span></p>
<p><span class="font0">Q_</span></p>
<p><span class="font2">0.4</span></p>
<p><span class="font2">0.3</span></p>
<p><span class="font2">0.2</span></p>
<p><span class="font2">0.i</span></p>
<p><span class="font2">0</span></p>
<div style="border-bottom:solid;"><img src="pidinet_files/pidinet-20.jpg" alt="" style="width:213pt;height:207pt;">
<p><span class="font1">[F=.803] Human</span></p>
<p><span class="font1">[F=.811] RCF-MS (2019)</span></p>
<p><span class="font1">[F=.810] PiDiNet-MS (Ours)</span></p>
<p><span class="font1">[F=.807] PiDiNet (Ours)</span></p>
<p><span class="font1">[F=.800] PiDiNet-L (Ours)</span></p>
<p><span class="font1">[F=.798] PiDiNet-Small (Ours)</span></p>
<p><span class="font1">[F=.793] PiDiNet-Small-L (Ours)</span></p>
<p><span class="font1">[F=.793] COB (2017)</span></p>
<p><span class="font1">[F=.789] PiDiNet-Tiny (Ours)</span></p>
<p><span class="font1">[F=.788] HED (2017)</span></p>
<p><span class="font1">[F=.787] PiDiNet-Tiny-L (Ours)</span></p>
<p><span class="font1">[F=.767] HFL (2015)</span></p>
<p><span class="font1">[F=.757] DeepContour (2015)</span></p>
<p><span class="font1">[F=.753] DeepEdge (2015)</span></p>
<p><span class="font1">[F=.746] OEF (2015)</span></p>
<p><span class="font1">[F=.743] SE (2013)</span></p>
<p><span class="font1">[F=.729] gPb-UCM (2011)</span></p>
<p><span class="font1">[F=.672] Pb (2004)</span></p>
<p><span class="font1">[F=.611] Canny (1986)</span></p>
<p><span class="font1">[F=.539] Sobel (1972)</span></p>
<p><span class="font1">[F=.483] Roberts (1963)</span></p>
<p><span class="font2">0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.2 &nbsp;&nbsp;&nbsp;&nbsp;0.3 &nbsp;&nbsp;&nbsp;&nbsp;0.4 &nbsp;&nbsp;&nbsp;&nbsp;0.5 &nbsp;&nbsp;&nbsp;&nbsp;0.6 &nbsp;&nbsp;&nbsp;&nbsp;0.7 &nbsp;&nbsp;&nbsp;&nbsp;0.8 &nbsp;&nbsp;&nbsp;&nbsp;0.9 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1</span></p>
<p><span class="font2">Recall</span></p>
</div><br clear="all">
<p><a name="bookmark70"></a><span class="font24">Figure 8. Precision-Recall curves of our models and some competitors on BSDS500 dataset.</span></p>
<p><span class="font25">as shown in the declines in the second half of the curves. In the following experiments, we only use the tiny, small, and normal versions of PiDiNet, dubbed as PiDiNet-Tiny, PiDiNet-Small and PiDiNet respectively.</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark71"></a><span class="font25" style="font-weight:bold;">5.4. Comparison with State-of-the-arts</span></h4></li></ul>
<p><span class="font25" style="font-weight:bold;">On BSDS500 dataset. </span><span class="font25">We compare our methods with prior edge detection approaches including both traditional ones and recently proposed CNN based ones, as summarized in Table</span><a href="#bookmark69"><span class="font25"> 5 </span></a><span class="font25">and Fig.</span><a href="#bookmark70"><span class="font25"> 8.</span></a><span class="font25"> Firstly, we notice that our baseline model can even achieve comparable results, </span><span class="font25" style="font-style:italic;">i.e.</span><span class="font25">, with ODS of 0.798 and OIS of 0.816, already beating most CNN based models like CED </span><a href="#bookmark17"><span class="font25">[55]</span></a><span class="font25">, DeepBoundary </span><a href="#bookmark14"><span class="font25">[23]</span></a><span class="font25"> and HED </span><a href="#bookmark16"><span class="font25">[60]</span></a><span class="font25">. With PDC, PiDiNet can further boost the performance with ODS of 0.807, being the same level as the</span></p>
<div>
<p><a name="bookmark72"></a><span class="font24"><a name="bookmark73"></a>Table 6. Comparison with other methods on NYUD dataset. </span><span class="font12">* </span><span class="font24">indicates the speeds with our implementations based on a NVIDIA RTX 2080 Ti GPU.</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font23">Methods</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">ODS</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">OIS</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">ODS</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">OIS</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">ODS</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">OIS</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">FPS</span></p></td></tr>
<tr><td>
<p><span class="font23">gPb-UCM </span><a href="#bookmark53"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.632</span></p></td><td>
<p><span class="font23">.661</span></p></td><td></td><td></td><td></td><td></td><td>
<p><span class="font23">1/360</span></p></td></tr>
<tr><td>
<p><span class="font23">gPb+NG </span><a href="#bookmark74"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.687</span></p></td><td>
<p><span class="font23">.716</span></p></td><td></td><td></td><td></td><td></td><td>
<p><span class="font23">1/375</span></p></td></tr>
<tr><td>
<p><span class="font23">SE </span><a href="#bookmark23"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.695</span></p></td><td>
<p><span class="font23">.708</span></p></td><td></td><td></td><td></td><td></td><td>
<p><span class="font23">5</span></p></td></tr>
<tr><td>
<p><span class="font23">SE+NG+</span><a href="#bookmark27"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.710</span></p></td><td>
<p><span class="font23">.723</span></p></td><td></td><td></td><td></td><td></td><td>
<p><span class="font23">1/15</span></p></td></tr>
<tr><td></td><td colspan="2">
<p><span class="font23">RGB</span></p></td><td colspan="2">
<p><span class="font23">HHA</span></p></td><td>
<p><span class="font23">RGB-</span></p></td><td>
<p><span class="font23">HHA</span></p></td><td></td></tr>
<tr><td>
<p><span class="font23">HED </span><a href="#bookmark16"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.720</span></p></td><td>
<p><span class="font23">.734</span></p></td><td>
<p><span class="font23">.682</span></p></td><td>
<p><span class="font23">.695</span></p></td><td>
<p><span class="font23">.746</span></p></td><td>
<p><span class="font23">.761</span></p></td><td>
<p><span class="font23">62</span><span class="font17">，</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">LPCB </span><a href="#bookmark36"><span class="font23">[]</span></a></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.739</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.754</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.707</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.719</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.762</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.778</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">-</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">RCF </span><a href="#bookmark18"><span class="font23">[]</span></a></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.743</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.757</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.703</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.717</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.765</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.780</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">52</span><span class="font17">*</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">AMH-Net </span><a href="#bookmark35"><span class="font23">[ 1]</span></a></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.744</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.758</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.716</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.729</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.771</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.786</span></p></td><td style="vertical-align:bottom;">
<p><span class="font23">-</span></p></td></tr>
<tr><td>
<p><span class="font23">BDCN </span><a href="#bookmark30"><span class="font23">[18]</span></a></p></td><td>
<p><span class="font23">.748</span></p></td><td>
<p><span class="font23">.763</span></p></td><td>
<p><span class="font23">.707</span></p></td><td>
<p><span class="font23">.719</span></p></td><td>
<p><span class="font23">.765</span></p></td><td>
<p><span class="font23">.781</span></p></td><td>
<p><span class="font23">33</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet</span></p></td><td>
<p><span class="font23">.733</span></p></td><td>
<p><span class="font23">.747</span></p></td><td>
<p><span class="font23">.715</span></p></td><td>
<p><span class="font23">.728</span></p></td><td>
<p><span class="font23">.756</span></p></td><td>
<p><span class="font23">.773</span></p></td><td>
<p><span class="font23">62</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-L</span></p></td><td>
<p><span class="font23">.728</span></p></td><td>
<p><span class="font23">.741</span></p></td><td>
<p><span class="font23">.709</span></p></td><td>
<p><span class="font23">.722</span></p></td><td>
<p><span class="font23">.754</span></p></td><td>
<p><span class="font23">.770</span></p></td><td>
<p><span class="font23">88</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-Small</span></p></td><td>
<p><span class="font23">.726</span></p></td><td>
<p><span class="font23">.741</span></p></td><td>
<p><span class="font23">.705</span></p></td><td>
<p><span class="font23">.719</span></p></td><td>
<p><span class="font23">.750</span></p></td><td>
<p><span class="font23">.767</span></p></td><td>
<p><span class="font23">115</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-Small-L</span></p></td><td>
<p><span class="font23">.721</span></p></td><td>
<p><span class="font23">.736</span></p></td><td>
<p><span class="font23">.701</span></p></td><td>
<p><span class="font23">.713</span></p></td><td>
<p><span class="font23">.746</span></p></td><td>
<p><span class="font23">.763</span></p></td><td>
<p><span class="font23">165</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-Tiny</span></p></td><td>
<p><span class="font23">.721</span></p></td><td>
<p><span class="font23">.736</span></p></td><td>
<p><span class="font23">.700</span></p></td><td>
<p><span class="font23">.714</span></p></td><td>
<p><span class="font23">.745</span></p></td><td>
<p><span class="font23">.763</span></p></td><td>
<p><span class="font23">140</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-Tiny-L</span></p></td><td>
<p><span class="font23">.714</span></p></td><td>
<p><span class="font23">.729</span></p></td><td>
<p><span class="font23">.693</span></p></td><td>
<p><span class="font23">.706</span></p></td><td>
<p><span class="font23">.741</span></p></td><td>
<p><span class="font23">.759</span></p></td><td>
<p><span class="font23">206</span><span class="font17">*</span></p></td></tr>
</table>
</div><br clear="all">
<p><span class="font25">recently proposed RCF </span><a href="#bookmark18"><span class="font25">[31]</span></a><span class="font25"> while still achieving nearly 100 FPS. The fastest version PiDiNet-Tiny-L, can also achieve comparable prediction performance with more than 200 FPS, further demonstrating the effectiveness of our methods. Noting all of our modes are trained from scratch using the same amount of training data as in RCF, LPCB, BDCN, </span><span class="font25" style="font-style:italic;">etc. (i.e.,</span><span class="font25"> the training and validation set, mixed with the VOC dataset), without the ImageNet pretraining. We also show some qualitative results in Figure</span><a href="#bookmark75"><span class="font25"> 9.</span></a><span class="font25"> A more detailed comparison in terms of network complexity, running efficiency and accuracy can be seen in Fig.</span><a href="#bookmark67"><span class="font25"> 7.</span></a></p>
<p><span class="font25" style="font-weight:bold;">On NYUD dataset. </span><span class="font25">The comparison results on the NYUD dataset are illustrated on Table</span><a href="#bookmark72"><span class="font25"> 6.</span></a><span class="font25"> Following the prior works, we get the ‘RGB-HHA’ results by averaging the output edge maps from RGB image and HHA image to get the final edge map. The quantitative comparison shows that PiDiNets can still achieve highly comparable results among the state-of-the-art methods while being efficient. Please refer to the appendix for the Precision-Recall curves. </span><span class="font25" style="font-weight:bold;">On Multicue dataset. </span><span class="font25">We also record the evaluation results on Multicue dataset and the comparison results with other methods are shown on Table</span><a href="#bookmark76"><span class="font25"> 7.</span></a><span class="font25"> Still, PiDiNets achieve promising results with high efficiencies.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark77"></a><span class="font26" style="font-weight:bold;">6. Conclusion</span></h3></li></ul>
<p><span class="font25">In conclusion, the contribution in this paper is three-fold: Firstly, we derive the pixel difference convolution which integrates the wisdom from the traditional edge detectors and the advantages of the deep CNNs, leading to robust and accurate edge detection. Secondly, we propose a highly efficient architecture named PiDiNet based on pixel difference convolution, which are memory friendly and with high inference speed. Furthermore, PiDiNet can be trained</span></p>
<div>
<p><a name="bookmark76"></a><span class="font24">Table 7. Comparison with other methods on Multicue dataset. </span><span class="font12">* </span><span class="font24">indicates the speeds with our implementations based on a NVIDIA RTX 2080 Ti GPU.</span></p>
<table border="1">
<tr><td rowspan="2">
<p><span class="font23">Method</span></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font23">Boundary</span></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font23">Edge</span></p></td><td rowspan="2">
<p><span class="font23">FPS</span></p></td></tr>
<tr><td>
<p><span class="font23">ODS</span></p></td><td>
<p><span class="font23">OIS</span></p></td><td>
<p><span class="font23">ODS</span></p></td><td>
<p><span class="font23">OIS</span></p></td></tr>
<tr><td>
<p><span class="font23">Human </span><a href="#bookmark55"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.760(.017)</span></p></td><td></td><td>
<p><span class="font23">.750 (.024)</span></p></td><td></td><td></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font23">Multicue </span><a href="#bookmark55"><span class="font23">[]</span></a></p></td><td style="vertical-align:bottom;">
<p><span class="font23">.720(.014)</span></p></td><td></td><td style="vertical-align:bottom;">
<p><span class="font23">.830 (.002)</span></p></td><td></td><td style="vertical-align:bottom;">
<p><span class="font23">-</span></p></td></tr>
<tr><td>
<p><span class="font23">HED</span><a href="#bookmark16"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.814(.011)</span></p></td><td>
<p><span class="font23">.822 (.008)</span></p></td><td>
<p><span class="font23">.851 (.014)</span></p></td><td>
<p><span class="font23">.864 (.011)</span></p></td><td>
<p><span class="font23">18</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">RCF</span><a href="#bookmark18"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.817 (.004)</span></p></td><td>
<p><span class="font23">.825 (.005)</span></p></td><td>
<p><span class="font23">.857 (.004)</span></p></td><td>
<p><span class="font23">.862 (.004)</span></p></td><td>
<p><span class="font23">15</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">BDCN</span><a href="#bookmark30"><span class="font23">[]</span></a></p></td><td>
<p><span class="font23">.836 (.001)</span></p></td><td>
<p><span class="font23">.846 (.003)</span></p></td><td>
<p><span class="font23">.891 (.001)</span></p></td><td>
<p><span class="font23">.898 (.002)</span></p></td><td>
<p><span class="font23">9</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet</span></p></td><td>
<p><span class="font23">.818 (.003)</span></p></td><td>
<p><span class="font23">.830 (.005)</span></p></td><td>
<p><span class="font23">.855 (.007)</span></p></td><td>
<p><span class="font23">.860 (.005)</span></p></td><td>
<p><span class="font23">17</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-L</span></p></td><td>
<p><span class="font23">.810(.005)</span></p></td><td>
<p><span class="font23">.822 (.002)</span></p></td><td>
<p><span class="font23">.854 (.007)</span></p></td><td>
<p><span class="font23">.860 (.004)</span></p></td><td>
<p><span class="font23">23</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-Small</span></p></td><td>
<p><span class="font23">.812(.004)</span></p></td><td>
<p><span class="font23">.825 (.004)</span></p></td><td>
<p><span class="font23">.858 (.007)</span></p></td><td>
<p><span class="font23">.863 (.004)</span></p></td><td>
<p><span class="font23">31</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-Small-L</span></p></td><td>
<p><span class="font23">.805 (.007)</span></p></td><td>
<p><span class="font23">.818 (.002)</span></p></td><td>
<p><span class="font23">.854 (.007)</span></p></td><td>
<p><span class="font23">.860 (.004)</span></p></td><td>
<p><span class="font23">44</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-Tiny</span></p></td><td>
<p><span class="font23">.807 (.007)</span></p></td><td>
<p><span class="font23">.819(.004)</span></p></td><td>
<p><span class="font23">.856 (.006)</span></p></td><td>
<p><span class="font23">.862 (.003)</span></p></td><td>
<p><span class="font23">43</span><span class="font17">*</span></p></td></tr>
<tr><td>
<p><span class="font23">PiDiNet-Tiny-L</span></p></td><td>
<p><span class="font23">.798 (.007)</span></p></td><td>
<p><span class="font23">.811 (.005)</span></p></td><td>
<p><span class="font23">.854 (.008)</span></p></td><td>
<p><span class="font23">.861 (.004)</span></p></td><td>
<p><span class="font23">56</span><span class="font17">*</span></p></td></tr>
</table>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-21.jpg" alt="" style="width:39pt;height:133pt;">
<p><span class="font2">Image</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-22.jpg" alt="" style="width:196pt;height:133pt;">
<p><span class="font2">GT &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RCF &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CED &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BDCN PiDiNet</span></p>
<p><a name="bookmark75"></a><span class="font24">Figure 9. A qualitative comparison of network outputs with some other methods, including RCF </span><a href="#bookmark18"><span class="font24">[31]</span></a><span class="font24">, CED </span><a href="#bookmark17"><span class="font24">[55]</span></a><span class="font24"> and BDCN </span><a href="#bookmark30"><span class="font24">[18]</span></a><span class="font24">.</span></p>
</div><br clear="all">
<p><span class="font25">from scratch only using limited data samples, while achieving human-level performances, breaking the convention that high performance CNN based edge detectors usually need a backbone pretrained on large scale dataset. Thirdly, we conduct extensive experiments on BSDS500, NYUD, and Multicue datasets for edge detection. We believe that PiDiNet has created new state-of-the-art performances considering both accuracy and efficiency.</span></p>
<p><span class="font25" style="font-weight:bold;">Future Work. </span><span class="font25">As discussed in Section</span><a href="#bookmark3"><span class="font25"> 1,</span></a><span class="font25"> edge detection is a low level task for many mid- or high-level vision tasks like semantic segmentation and object detection. Also, some low level tasks like salient object detection may also benefit from the image boundary information. We hope pixel difference convolution and the proposed PiDiNet can go further and be useful in these related tasks.</span></p>
<p><span class="font25" style="font-weight:bold;">Acknowledgement. </span><span class="font25">This work was partially supported by the Academy of Finland under grant 331883 and the National Natural Science Foundation of China under Grant 61872379,62022091, and 71701205. The authors also wish to acknowledge CSC IT Center for Science, Finland, for computational resources.</span></p>
<h3><a name="bookmark53"></a><span class="font26" style="font-weight:bold;"><a name="bookmark78"></a>References</span></h3>
<ul style="list-style:none;"><li>
<p><span class="font24">[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-tendra Malik. Contour detection and hierarchical image segmentation. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 33(5):898-916, 2011.</span><a href="#bookmark47"><span class="font24"> 5,</span></a><a href="#bookmark73"><span class="font24"> 8,</span></a><a href="#bookmark79"><span class="font24"> 13,</span></a><a href="#bookmark80"><span class="font24"> 14</span></a></p></li>
<li>
<p><a name="bookmark13"></a><span class="font24">[2] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. Deepedge: A multi-scale bifurcated deep network for topdown contour detection. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> pages 4380-4389, 2015. </span><a href="#bookmark1"><span class="font24">1,</span></a><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark82"><span class="font24"> 3,</span></a><a href="#bookmark65"><span class="font24"> 7</span></a></p></li>
<li>
<p><a name="bookmark31"></a><span class="font24">[3] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. High-for-low and low-for-high: Efficient boundary detection from deep object features and its applications to high-level vision. In </span><span class="font24" style="font-style:italic;">ICCV</span><span class="font24">,pages504512, 2015.</span><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark82"><span class="font24"> 3,</span></a><a href="#bookmark65"><span class="font24">7</span></a></p></li>
<li>
<p><a name="bookmark12"></a><span class="font24">[4] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. Semantic segmentation with boundary neural fields. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages3602-3610, 2016.</span><a href="#bookmark1"><span class="font24"> 1</span></a></p></li>
<li>
<p><a name="bookmark5"></a><span class="font24">[5] John Canny. A computational approach to edge detection. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font24">, (6):679-698, 1986.</span><a href="#bookmark1"><span class="font24"> 1,</span></a><a href="#bookmark82"><span class="font24"> 3,</span></a><a href="#bookmark65"><span class="font24"> 7</span></a></p></li>
<li>
<p><a name="bookmark8"></a><span class="font24">[6] Ming-Ming Cheng, Ziming Zhang, Wen-Yan Lin, and Philip Torr. Bing: Binarized normed gradients for objectness estimation at 300fps. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 3286-3293, 2014.</span><a href="#bookmark1"><span class="font24"> 1</span></a></p></li>
<li>
<p><a name="bookmark39"></a><span class="font24">[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 248-255. Ieee, 2009.</span><a href="#bookmark81"><span class="font24"> 2</span></a></p></li>
<li>
<p><a name="bookmark36"></a><span class="font24">[8] Ruoxi Deng, Chunhua Shen, Shengjun Liu, Huibing Wang, and Xinru Liu. Learning to predict crisp boundaries. In </span><span class="font24" style="font-style:italic;">ECCV</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> pages 562-578, 2018.</span><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark65"><span class="font24"> 7,</span></a><a href="#bookmark73"><span class="font24"> 8</span></a></p></li>
<li>
<p><a name="bookmark23"></a><span class="font24">[9] Piotr Dollar and C Lawrence Zitnick. Fast edge detection using structured forests. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font24">, 37(8):1558-1570, 2015.</span><a href="#bookmark1"><span class="font24"> 1,</span></a><a href="#bookmark82"><span class="font24"> 3,</span></a><span class="font24"> </span><a href="#bookmark65"><span class="font24">7,</span></a><a href="#bookmark73"><span class="font24"> 8,</span></a><a href="#bookmark79"><span class="font24"> 13</span></a></p></li>
<li>
<p><a name="bookmark10"></a><span class="font24">[10] James H Elder and Richard M Goldberg. Image editing in the contour domain. In </span><span class="font24" style="font-style:italic;">Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 98CB36231)</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> pages 374-381. IEEE, 1998. </span><a href="#bookmark1"><span class="font24">1</span></a></p></li>
<li>
<p><a name="bookmark7"></a><span class="font24">[11] Vittorio Ferrari, Loic Fevrier, Frederic Jurie, and Cordelia Schmid. Groups of adjacent contour segments for object detection. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence,</span><span class="font24"> 30(1):36-51, 2008.</span><a href="#bookmark1"><span class="font24"> 1</span></a></p></li>
<li>
<p><a name="bookmark26"></a><span class="font24">[12] David R Martin Charless C Fowlkes and Jitendra Malik. Learning to detect natural image boundaries using brightness and texture. </span><span class="font24" style="font-style:italic;">NeurIPS</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 2002.</span><a href="#bookmark1"><span class="font24"> 1</span></a></p></li>
<li>
<p><a name="bookmark83"></a><span class="font24">[13] Shang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng, Chengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly efficient salient object detection with 100k parameters. In </span><span class="font24" style="font-style:italic;">ECCV</span><span class="font24">, pages 702-721. Springer, 2020.</span><a href="#bookmark82"><span class="font24"> 3,</span></a><a href="#bookmark84"><span class="font24"> 4</span></a></p></li>
<li>
<p><a name="bookmark74"></a><span class="font24">[14] Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik. Perceptual organization and recognition of indoor scenes from rgb-d images. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 564-571, 2013.</span><a href="#bookmark73"><span class="font24"> 8,</span></a><a href="#bookmark79"><span class="font24"> 13</span></a></p></li>
<li>
<p><a name="bookmark27"></a><span class="font24">[15] Saurabh Gupta, Ross Girshick, Pablo Arbelaez, and Jiten-dra Malik. Learning rich features from rgb-d images for object detection and segmentation. In </span><span class="font24" style="font-style:italic;">ECCV</span><span class="font24">, pages 345-360. Springer, 2014.</span><a href="#bookmark1"><span class="font24"> 1,</span></a><a href="#bookmark73"><span class="font24"> 8</span></a></p></li>
<li>
<p><a name="bookmark22"></a><span class="font24">[16] Sam Hallman and Charless C Fowlkes. Oriented edge forests for boundary detection. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 1732-1740, 2015. </span><a href="#bookmark1"><span class="font24">1,</span></a><a href="#bookmark65"><span class="font24">7,</span></a><a href="#bookmark79"><span class="font24"> 13</span></a></p></li>
<li>
<p><a name="bookmark85"></a><span class="font24">[17] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In </span><span class="font24" style="font-style:italic;">ICLR</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 2016.</span><a href="#bookmark84"><span class="font24"> 4</span></a></p></li>
<li>
<p><a name="bookmark30"></a><span class="font24">[18] Jianzhong He, Shiliang Zhang, Ming Yang, Yanhu Shan, and Tiejun Huang. Bi-directional cascade network for perceptual edge detection. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 3828-3837, 2019.</span><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark47"><span class="font24"> 5,</span></a><a href="#bookmark65"><span class="font24"> 7,</span></a><a href="#bookmark73"><span class="font24"> 8</span></a></p></li>
<li>
<p><a name="bookmark40"></a><span class="font24">[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 770-778, 2016.</span><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark82"><span class="font24"> 3,</span></a><a href="#bookmark84"><span class="font24"> 4</span></a></p></li>
<li>
<p><a name="bookmark41"></a><span class="font24">[20] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-dreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. </span><span class="font24" style="font-style:italic;">arXiv preprint arXiv:1704.04861</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 2017.</span><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark84"><span class="font24"> 4</span></a></p></li>
<li>
<p><a name="bookmark86"></a><span class="font24">[21] Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios Sav-vides. Local binary convolutional neural networks. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 19-28, 2017.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark58"></a><span class="font24">[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, </span><span class="font24" style="font-style:italic;">ICLR</span><span class="font24">, 2015.</span><a href="#bookmark47"><span class="font24"> 5</span></a></p></li>
<li>
<p><a name="bookmark14"></a><span class="font24">[23] Iasonas Kokkinos. Pushing the boundaries of boundary detection using deep learning. </span><span class="font24" style="font-style:italic;">arXiv preprint arXiv:1511.07386</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 2015.</span><a href="#bookmark1"><span class="font24"> 1,</span></a><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark65"><span class="font24"> 7</span></a></p></li>
<li>
<p><a name="bookmark87"></a><span class="font24">[24] Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 6129-6138, 2017.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark88"></a><span class="font24">[25] Gen Li and Joongkyu Kim. Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation. In </span><span class="font24" style="font-style:italic;">BMVC</span><span class="font24">, page 259. BMVA Press, 2019.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark89"></a><span class="font24">[26] Jiang-Jiang Liu, Qibin Hou, and Ming-Ming Cheng. Dynamic feature integration for simultaneous detection of salient object, edge, and skeleton. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Image Processing</span><span class="font24">, 29:8652-8667, 2020.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark90"></a><span class="font24">[27] Li Liu, Jie Chen, Paul Fieguth, Guoying Zhao, Rama Chel-lappa, and Matti Pietikainen. From bow to cnn: Two decades of texture representation for texture classification. </span><span class="font24" style="font-style:italic;">International Journal of Computer Vision</span><span class="font24">, 127(1):74-109, 2019.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark91"></a><span class="font24">[28] Li Liu, Paul Fieguth, Gangyao Kuang, and Hongbin Zha. Sorted random projections for robust texture classification. In </span><span class="font24" style="font-style:italic;">ICCV</span><span class="font24">, pages 391-398. IEEE, 2011.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark6"></a><span class="font24">[29] Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietikainen. Deep learning for generic object detection: A survey. </span><span class="font24" style="font-style:italic;">International Journal ofComputer Vision</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 128(2):261-318, 2020.</span><a href="#bookmark1"><span class="font24"> 1</span></a></p></li>
<li>
<p><a name="bookmark92"></a><span class="font24">[30] Li Liu, Lingjun Zhao, Yunli Long, Gangyao Kuang, and Paul Fieguth. Extended local binary patterns for texture classification. </span><span class="font24" style="font-style:italic;">Image and Vision Computing</span><span class="font24">, 30(2):86-99, 2012.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark18"></a><span class="font24">[31] Yun Liu, Ming-Ming Cheng, Xiaowei Hu, Jia-Wang Bian, Le Zhang, Xiang Bai, and Jinhui Tang. Richer convolutional features for edge detection. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font24">, 41(8):1939-1946, 2019. </span><a href="#bookmark1"><span class="font24">1,</span></a><a href="#bookmark81"><span class="font24">2,</span></a><a href="#bookmark84"><span class="font24"> 4,</span></a><a href="#bookmark47"><span class="font24"> 5,</span></a><a href="#bookmark65"><span class="font24"> 7,</span></a><a href="#bookmark73"><span class="font24"> 8,</span></a><a href="#bookmark79"><span class="font24">13</span></a></p></li>
<li>
<p><a name="bookmark93"></a><span class="font24">[32] Yu Liu and Michael S Lew. Learning relaxed deep supervision for better edge detection. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> pages 231-240, 2016.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark32"></a><span class="font24">[33] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> pages 3431-3440, 2015.</span><a href="#bookmark81"><span class="font24"> 2</span></a></p></li>
<li>
<p><a name="bookmark94"></a><span class="font24">[34] Shangzhen Luan, Chen Chen, Baochang Zhang, Jungong Han, and Jianzhuang Liu. Gabor convolutional networks. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Image Processing</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 27(9):4357-4366, 2018.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark95"></a><span class="font24">[35] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In </span><span class="font24" style="font-style:italic;">ECCV</span><span class="font24">, pages 116-131, 2018.</span><a href="#bookmark84"><span class="font24"> 4</span></a></p></li>
<li>
<p><a name="bookmark34"></a><span class="font24">[36] Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbelaez, and Luc Van Gool. Convolutional oriented boundaries. In </span><span class="font24" style="font-style:italic;">ECCV</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> pages 580-596. Springer, 2016.</span><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark65"><span class="font24"> 7</span></a></p></li>
<li>
<p><a name="bookmark25"></a><span class="font24">[37] David R Martin, Charless C Fowlkes, and Jitendra Malik. Learning to detect natural image boundaries using local brightness, color, and texture cues. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 26(5):530-549, 2004.</span><a href="#bookmark1"><span class="font24"> 1,</span></a><a href="#bookmark65"><span class="font24"> 7</span></a></p></li>
<li>
<p><a name="bookmark96"></a><span class="font24">[38] Sachin Mehta, Mohammad Rastegari, Linda Shapiro, and Hannaneh Hajishirzi. Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 9190-9200, 2019.</span><a href="#bookmark82"><span class="font24"> 3,</span></a><a href="#bookmark84"><span class="font24"> 4</span></a></p></li>
<li>
<p><a name="bookmark55"></a><span class="font24">[39] David A Mely, Junkyung Kim, Mason McGill, Yuliang Guo, and Thomas Serre. A systematic comparison between visual cues for boundary detection. </span><span class="font24" style="font-style:italic;">Vision Research</span><span class="font24">, 120:93-107, 2016.</span><a href="#bookmark47"><span class="font24"> 5,</span></a><a href="#bookmark73"><span class="font24"> 8</span></a></p></li>
<li>
<p><a name="bookmark56"></a><span class="font24">[40] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 891-898,</span></p></li></ul>
<ul style="list-style:none;"><li>
<p class="font24">2014. <a href="#bookmark47">5,</a><a href="#bookmark79"> 13</a></p></li></ul>
<ul style="list-style:none;"><li>
<p><a name="bookmark11"></a><span class="font24">[41] R Muthukrishnan and Miyilsamy Radha. Edge detection techniques for image segmentation. </span><span class="font24" style="font-style:italic;">International Journal of Computer Science &amp;&nbsp;Information Technology</span><span class="font24">, 3(6):259, 2011.</span><a href="#bookmark1"><span class="font24"> 1</span></a></p></li>
<li>
<p><a name="bookmark97"></a><span class="font24">[42] Timo Ojala, Matti Pietikainen, and Topi Maenpaa. Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 24(7):971-987, 2002.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark98"></a><span class="font24">[43] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. Enet: A deep neural network architecture for real-time semantic segmentation. </span><span class="font24" style="font-style:italic;">arXiv preprint arXiv:1606Q2147</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 2016.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark57"></a><span class="font24">[44] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In </span><span class="font24" style="font-style:italic;">NeurIPS</span><span class="font24">, pages 8024-8035, 2019.</span><a href="#bookmark47"><span class="font24"> 5</span></a></p></li>
<li>
<p><a name="bookmark99"></a><span class="font24">[45] Xavier Soria Poma, Edgar Riba, and Angel Sappa. Dense extreme inception network: Towards a robust cnn model for edge detection. In </span><span class="font24" style="font-style:italic;">WACV</span><span class="font24">, pages 1923-1932, 2020.</span><a href="#bookmark82"><span class="font24"> 3,</span></a><a href="#bookmark84"><span class="font24"> 4</span></a></p></li>
<li>
<p><a name="bookmark21"></a><span class="font24">[46] Judith MS Prewitt. Object enhancement and extraction. </span><span class="font24" style="font-style:italic;">Picture processing and Psychopictorics</span><span class="font24">, 10(1):15-19, 1970.</span><a href="#bookmark1"><span class="font24"> 1</span></a></p></li>
<li>
<p><a name="bookmark15"></a><span class="font24">[47] Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, and Zhi-jiang Zhang. Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> pages 3982-3991, 2015.</span><a href="#bookmark1"><span class="font24"> 1,</span></a><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark65"><span class="font24"> 7</span></a></p></li>
<li>
<p><a name="bookmark54"></a><span class="font24">[48] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font24">, 22(8):888-905, 2000.</span><a href="#bookmark47"><span class="font24"> 5,</span></a><a href="#bookmark79"><span class="font24"> 13</span></a></p></li>
<li>
<p><a name="bookmark38"></a><span class="font24">[49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In </span><span class="font24" style="font-style:italic;">ICLR</span><span class="font24">,</span></p></li></ul>
<ul style="list-style:none;"><li>
<p class="font24">2015. <a href="#bookmark81">2</a></p></li></ul>
<ul style="list-style:none;"><li>
<p><a name="bookmark20"></a><span class="font24">[50] Irwin Sobel and Gary Feldman. A 3x3 isotropic gradient operator for image processing. </span><span class="font24" style="font-style:italic;">A Talk at The Stanford Artificial Project in</span><span class="font24">, pages 271-272, 1968.</span><a href="#bookmark1"><span class="font24"> 1</span></a></p></li>
<li>
<p><a name="bookmark100"></a><span class="font24">[51] Zhuo Su, Linpu Fang, Wenxiong Kang, Dewen Hu, Matti Pietikainen, and Li Liu. Dynamic group convolution for accelerating convolutional neural networks. In </span><span class="font24" style="font-style:italic;">ECCV</span><span class="font24">, pages 138-155. Springer, 2020.</span><a href="#bookmark84"><span class="font24"> 4</span></a></p></li>
<li>
<p><a name="bookmark101"></a><span class="font24">[52] Zhuo Su, Matti Pietikainen, and Li Liu. Bird: Learning binary and illumination robust descriptor for face recognition. In </span><span class="font24" style="font-style:italic;">BMVC</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> page 102, 2019.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark19"></a><span class="font24">[53] Vincent Torre and Tomaso A Poggio. On edge detection. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> (2):147-163, 1986.</span><a href="#bookmark1"><span class="font24"> 1</span></a></p></li>
<li>
<p><a name="bookmark9"></a><span class="font24">[54] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-ers, and Arnold WM Smeulders. Selective search for object recognition. </span><span class="font24" style="font-style:italic;">International Journal of Computer Vision</span><span class="font24">, 104(2):154-171, 2013.</span><a href="#bookmark1"><span class="font24"> 1</span></a></p></li>
<li>
<p><a name="bookmark17"></a><span class="font24">[55] Yupei Wang, Xin Zhao, and Kaiqi Huang. Deep crisp boundaries. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 3892-3900, 2017.</span><a href="#bookmark1"><span class="font24"> 1,</span></a><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark65"><span class="font24"> 7,</span></a><a href="#bookmark73"><span class="font24"> 8</span></a></p></li>
<li>
<p><a name="bookmark68"></a><span class="font24">[56] Jan Kristanto Wibisono and Hsueh-Ming Hang. Fined: Fast inference network for edge detection. </span><span class="font24" style="font-style:italic;">arXiv preprint arXiv:2012.08392</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 2020.</span><a href="#bookmark82"><span class="font24"> 3,</span></a><a href="#bookmark84"><span class="font24"> 4,</span></a><a href="#bookmark65"><span class="font24"> 7</span></a></p></li>
<li>
<p><a name="bookmark102"></a><span class="font24">[57] Jan Kristanto Wibisono and Hsueh-Ming Hang. Traditional method inspired deep neural network for edge detection. In </span><span class="font24" style="font-style:italic;">ICIP</span><span class="font24">, pages 678-682. IEEE, 2020.</span><a href="#bookmark82"><span class="font24"> 3,</span></a><a href="#bookmark84"><span class="font24"> 4</span></a></p></li>
<li>
<p><a name="bookmark103"></a><span class="font24">[58] Tianyi Wu, Sheng Tang, Rui Zhang, Juan Cao, and Yong-dong Zhang. Cgnet: A light-weight context guided network for semantic segmentation. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Image Processing</span><span class="font24">, 30:1169-1179, 2020.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark24"></a><span class="font24">[59] Ren Xiaofeng and Liefeng Bo. Discriminatively trained sparse code gradients for contour detection. In </span><span class="font24" style="font-style:italic;">NeurIPS</span><span class="font24">, pages 584-592, 2012.</span><a href="#bookmark1"><span class="font24"> 1,</span></a><a href="#bookmark65"><span class="font24"> 7</span></a></p></li>
<li>
<p><a name="bookmark16"></a><span class="font24">[60] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. </span><span class="font24" style="font-style:italic;">International Journal of Computer Vision</span><span class="font24">, 125(1-3):3-18, 2017.</span><a href="#bookmark1"><span class="font24"> 1,</span></a><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark84"><span class="font24"> 4,</span></a><a href="#bookmark47"><span class="font24"> 5,</span></a><a href="#bookmark65"><span class="font24"> 7,</span></a><a href="#bookmark73"><span class="font24"> 8</span></a></p></li>
<li>
<p><a name="bookmark35"></a><span class="font24">[61] Dan Xu, Wanli Ouyang, Xavier Alameda-Pineda, Elisa Ricci, Xiaogang Wang, and Nicu Sebe. Learning deep structured multi-scale features using attention-gated crfs for contour prediction. In </span><span class="font24" style="font-style:italic;">NeurIPS</span><span class="font24">, pages 3961-3970, 2017.</span><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark65"><span class="font24"> 7,</span></a><span class="font24"> </span><a href="#bookmark73"><span class="font24">8</span></a></p></li>
<li>
<p><a name="bookmark33"></a><span class="font24">[62] Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, and Ming-Hsuan Yang. Object contour detection with a fully convolutional encoder-decoder network. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font24">, pages 193-202, 2016.</span><a href="#bookmark81"><span class="font24"> 2,</span></a><a href="#bookmark65"><span class="font24"> 7</span></a></p></li>
<li>
<p><a name="bookmark104"></a><span class="font24">[63] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In </span><span class="font24" style="font-style:italic;">ECCV</span><span class="font17" style="font-style:italic;">, </span><span class="font24">pages325-341,2018.</span><a href="#bookmark82"><span class="font24"> 3,</span></a><a href="#bookmark84"><span class="font24">4</span></a></p></li>
<li>
<p><a name="bookmark105"></a><span class="font24">[64] Zitong Yu, Yunxiao Qin, Hengshuang Zhao, Xiaobai Li, and Guoying Zhao. Dual-cross central difference network for face anti-spoofing. In </span><span class="font24" style="font-style:italic;">IJCAI</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 2021.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark106"></a><span class="font24">[65] Zitong Yu, Jun Wan, Yunxiao Qin, Xiaobai Li, Stan Z Li, and Guoying Zhao. Nas-fas: Static-dynamic central difference network search for face anti-spoofing. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> 2021.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark107"></a><span class="font24">[66] Zitong Yu, Chenxu Zhao, Zezheng Wang, Yunxiao Qin, Zhuo Su, Xiaobai Li, Feng Zhou, and Guoying Zhao. Searching central difference convolutional networks for face anti-spoofing. In </span><span class="font24" style="font-style:italic;">CVPR</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> pages 5295-5305, 2020.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p></li>
<li>
<p><a name="bookmark108"></a><span class="font24">[67] Zitong Yu, Benjia Zhou, Jun Wan, Pichao Wang, Haoyu Chen, Xin Liu, Stan Z Li, and Guoying Zhao. Searching multi-rate and multi-modal temporal enhanced networks for gesture recognition. </span><span class="font24" style="font-style:italic;">IEEE Transactions on Image Processing</span><span class="font24">, 2021.</span><a href="#bookmark82"><span class="font24"> 3</span></a></p>
<div>
<p><span class="font1">CPDC</span></p><img src="pidinet_files/pidinet-23.jpg" alt="" style="width:44pt;height:44pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-24.jpg" alt="" style="width:48pt;height:48pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-25.jpg" alt="" style="width:48pt;height:48pt;">
</div><br clear="all">
<div>
<p><span class="font24">Figure 10. Selection of pixel pairs and convolution in CPDC.</span></p>
</div><br clear="all">
<div>
<h3><a name="bookmark109"></a><span class="font26" style="font-weight:bold;">7. Appendix</span></h3>
<h4><a name="bookmark110"></a><span class="font25" style="font-weight:bold;font-style:italic;">7.1.</span><span class="font25" style="font-weight:bold;"> Converting Pixel Difference Convolution （PDC） to Vanilla Convolution</span></h4>
<p><span class="font25">The main goal of the conversion is to make PDC as fast and memory efficient as as the vanilla convolution. As introduced in the main paper, the formulations of vanilla convolution and PDC can be written as:</span></p>
</div><br clear="all">
<div>
<p><span class="font17" style="font-style:italic;">kxk</span></p>
<p><span class="font17" style="font-style:italic;">y = &nbsp;&nbsp;&nbsp;&nbsp;= y^<sup>w</sup>i</span><span class="font17"> •旳,</span></p>
<p><span class="font17" style="font-style:italic;">i=l</span></p>
</div><br clear="all">
<div>
<p><span class="font25">（vanilla convolution）</span></p>
</div><br clear="all">
<div>
<p><span class="font26" style="font-style:italic;">y = &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font17" style="font-weight:bold;">= £ 凹•</span><span class="font21">（</span><span class="font25">X -</span><span class="font17" style="font-weight:bold;">葛），</span><span class="font21">（</span><span class="font25">PDC）</span></p>
</div><br clear="all">
<div>
<p><span class="font25">（5）</span></p>
<p><span class="font25">（6）</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-26.jpg" alt="" style="width:224pt;height:50pt;">
<p><span class="font24">Figure 11. Selection of pixel pairs and convolution in APDC.</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-27.jpg" alt="" style="width:218pt;height:138pt;">
<p><span class="font24">Figure 12. Selection of pixel pairs and convolution in RPDC.</span></p>
</div><br clear="all"></li></ul>
<p><span class="font25">where, </span><span class="font25" style="font-style:italic;">Xi</span><span class="font25"> and are the pixels in the current input local patch, </span><span class="font25" style="font-style:italic;">Wi</span><span class="font25"> is the weight in the # x # convolution kernel. </span><span class="font25" style="font-weight:bold;font-style:italic;">P</span><span class="font17"> = {（⑦*）,（⑦</span><span class="font25">2,</span><span class="font17">⑦获...，（⑦⑦人）} </span><span class="font25">is the set of pixel pairs picked from the local patch, and </span><span class="font25" style="font-style:italic;">m &lt;&nbsp;k x k.</span></p>
<p><span class="font25">The conversion from PDC to vanilla convolution can be done in both the training and inference phases.</span></p>
<p><span class="font25" style="font-weight:bold;">Conversion in the Training Phase. </span><span class="font25">Eq. 6 can be transformed to fit the form of Eq. 5, according to the selection strategies of the pixel pairs. Correspondingly, PDC can be converted to vanilla convolution by firstly transforming the kernel weights to a new set of kernel weights, followed by a vanilla convolutional operation. We will discuss Central PDC （CPDC）, Angular PDC （APDC） and Radial PDC （RPDC） respectively. The selection strategies of pixel pairs in the three PDC instances are shown in Fig. 10, Fig. 11 and Fig. 12. The transformations of the equations are as follows. For CPDC （Fig. 10）:</span></p>
<p><span class="font17" style="font-style:italic;">y</span><span class="font8"> =W1 </span><span class="font17">-（力 </span><span class="font8">1 </span><span class="font17">一 力</span><span class="font8">5）+ W2 </span><span class="font17">-（力</span><span class="font8">2 </span><span class="font17">一 力</span><span class="font8">5）+ </span><span class="font17" style="font-style:italic;">W3</span><span class="font17"> •（力</span><span class="font8">3 </span><span class="font17">一 力</span><span class="font8">5）</span></p>
<p><span class="font8">+ W<sub>4</sub> </span><span class="font17">•（力</span><span class="font8">4 -</span><span class="font17">力</span><span class="font8">5）+ </span><span class="font17">迎</span><span class="font8">6 </span><span class="font17">•（力</span><span class="font8">6 </span><span class="font17">-力</span><span class="font8">5）+ </span><span class="font17">迎</span><span class="font8">7 </span><span class="font17">•（力</span><span class="font8">7 -</span><span class="font17">力</span><span class="font8">5）</span></p>
<p><span class="font8">+ W8 </span><span class="font17">•（力</span><span class="font8">8 -</span><span class="font17">力</span><span class="font8">5）+ Wg </span><span class="font17">•（力</span><span class="font8">9 -</span><span class="font17">力</span><span class="font8">5）</span></p>
<p><span class="font17">=迎</span><span class="font8">1 </span><span class="font17">•力</span><span class="font8">1 </span><span class="font17">+迎</span><span class="font8">2 </span><span class="font17">•力</span><span class="font8">2 </span><span class="font17">+迎</span><span class="font8">3 •</span><span class="font17">力</span><span class="font8">3 +</span></p>
<p><span class="font17">+迎</span><span class="font8">4 </span><span class="font17">•力</span><span class="font8">4 </span><span class="font17">+迎</span><span class="font8">6 </span><span class="font17">•力</span><span class="font8">6 </span><span class="font17">+迎</span><span class="font8">7 •</span><span class="font17">力</span><span class="font8">7 +</span></p>
<p><span class="font17" style="font-style:italic;">Ws ■ Xs Wg ■ Xg</span></p>
<p><span class="font8">+ （- &nbsp;&nbsp;&nbsp;£ </span><span class="font8" style="font-style:italic;">脸</span><span class="font17">.力</span><span class="font24">5</span></p>
<p><span class="font8">/={1,2,3,4,6,7,8,9}</span></p>
<p><span class="font17">=而</span><span class="font8">1 </span><span class="font17">•力</span><span class="font8">1 </span><span class="font17">+ 而</span><span class="font8">2 </span><span class="font17">•力</span><span class="font8">2 </span><span class="font17">+ 而</span><span class="font8">3 </span><span class="font17">•力</span><span class="font8">3 + ... = &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font17" style="font-style:italic;">• Xi</span><span class="font24"> （7）</span></p>
<p><span class="font25">For APDC （Fig. 11）:</span></p>
<p><span class="font17" style="font-style:italic;">y</span><span class="font8"> =W1 </span><span class="font17">-（力 </span><span class="font8">1 -</span><span class="font17">力</span><span class="font8">2）+ W2 </span><span class="font17">-（力</span><span class="font8">2 -</span><span class="font17">力</span><span class="font8">3）+ W3 </span><span class="font17">•（力</span><span class="font8">3 -</span><span class="font17">力</span><span class="font8">6）</span></p>
<p><span class="font17">+迎</span><span class="font8">4 </span><span class="font17">•（力</span><span class="font8">4 </span><span class="font17">-力</span><span class="font8">1） +</span><span class="font17">迎</span><span class="font8">6 </span><span class="font17">•（力</span><span class="font8">6 -</span><span class="font17">力</span><span class="font8">9）+</span><span class="font17">迎</span><span class="font8">7 </span><span class="font17">•（力</span><span class="font8">7 -</span><span class="font17">力</span><span class="font8">4）</span></p>
<p><span class="font8">+ W8 </span><span class="font17">•（力</span><span class="font8">8 -</span><span class="font17">力</span><span class="font8">7）+ Wg </span><span class="font17">•（力</span><span class="font8">9 -</span><span class="font17">力</span><span class="font8">8）</span></p>
<p><span class="font8">=（W1 — W4）• </span><span class="font17" style="font-style:italic;">Xi +（W2 —</span><span class="font8"> W1） • </span><span class="font17" style="font-style:italic;">X2</span><span class="font8"> +（W3 —</span><span class="font17">迎</span><span class="font8">2）-</span><span class="font17">力</span><span class="font8">3</span></p>
<p><span class="font17" style="font-style:italic;">+（W<sub>4</sub></span><span class="font17"> 一 </span><span class="font17" style="font-style:italic;">W<sub>7</sub>） • X<sub>4</sub> +（W<sub>6</sub></span><span class="font17"> 一 </span><span class="font17" style="font-style:italic;">W<sub>3</sub>）</span><span class="font17"> •力</span><span class="font8">6 </span><span class="font17">+ （迎</span><span class="font8">7 </span><span class="font17">一 </span><span class="font8">W<sub>8</sub>） •</span><span class="font17">力</span><span class="font8">7</span></p>
<p><span class="font8">+（W8 </span><span class="font17">-迎</span><span class="font8">9）</span><span class="font17">•力</span><span class="font8">8 + （Wg -</span><span class="font17">力</span><span class="font8">6）- </span><span class="font17" style="font-style:italic;">Xg</span></p>
<p><span class="font24">+ 0 </span><span class="font17">•力</span><span class="font8">5</span></p>
<p><span class="font17">=而</span><span class="font8">1 </span><span class="font17">•力</span><span class="font8">1 </span><span class="font17">+ 而</span><span class="font8">2 </span><span class="font17">•力</span><span class="font8">2 </span><span class="font17">+ 而</span><span class="font8">3 </span><span class="font17">•力</span><span class="font8">3 + ... = £ &amp;</span><span class="font17">，<sup>Xi</sup></span><span class="font24"> </span><span class="font20">（</span><span class="font24">8</span><span class="font20">）</span></p>
<p><span class="font25">For RPDC （Fig. 12）:</span></p>
<p><span class="font17" style="font-style:italic;">y</span><span class="font8"> =W1 </span><span class="font17">-（力 </span><span class="font8">1 </span><span class="font17">一 力</span><span class="font8">7）+ </span><span class="font17" style="font-style:italic;">W3</span><span class="font17"> •（力</span><span class="font8">3 — </span><span class="font17">力</span><span class="font8">8）+ </span><span class="font17" style="font-style:italic;">35 -</span><span class="font17">（力</span><span class="font8">5 — </span><span class="font17">力</span><span class="font8">9）</span></p>
<p><span class="font8">+ W11 -</span><span class="font17" style="font-style:italic;">（XH - X<sub>12</sub>）</span><span class="font8"> + W15 - 015 - %14）</span></p>
<p><span class="font8">+ W<sub>2</sub>1 -</span><span class="font17" style="font-style:italic;">（X<sub>2</sub>1 -</span><span class="font17">力 </span><span class="font8">17）+ W<sub>23</sub> </span><span class="font17">•（力</span><span class="font8">23 -</span><span class="font17">力 </span><span class="font8">18）</span></p>
<p><span class="font8">+ W25 </span><span class="font17">•（力</span><span class="font8">25 -</span><span class="font17">力 </span><span class="font8">19）</span></p>
<p><span class="font17">=迎</span><span class="font8">1 </span><span class="font17">•力</span><span class="font8">1 </span><span class="font17">+迎</span><span class="font8">3 </span><span class="font17">•力</span><span class="font8">3 </span><span class="font17">+迎</span><span class="font8">5 </span><span class="font17">•力</span><span class="font8">5</span></p>
<p><span class="font8">+ （-W1） • </span><span class="font17" style="font-style:italic;">X<sub>7</sub></span><span class="font8"> + （-W3）,</span><span class="font17">力</span><span class="font8">8 </span><span class="font17">+ （—迎</span><span class="font8">5）,</span><span class="font17">力</span><span class="font8">9 +</span></p>
<p><span class="font8">+ Wn -</span><span class="font17">力 </span><span class="font8">11 + （-W11） •</span><span class="font17">力 </span><span class="font8">12 + （-W15）•</span><span class="font17">力 </span><span class="font8">14</span></p>
<p><span class="font8">+ W15 •</span><span class="font17">力 </span><span class="font8">15 + （-W21） •</span><span class="font17">力 </span><span class="font8">17 + （-W23）- ^18</span></p>
<p><span class="font8">+ （―W25）•</span><span class="font17">力 </span><span class="font8">19 + W21 -</span><span class="font17">力</span><span class="font8">21 + W23 -</span><span class="font17">力</span><span class="font8">23</span></p>
<p><span class="font8">+ W25 • %25 + &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;£ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font24">0 </span><span class="font17">•初</span></p>
<p><span class="font8">i={2,4,6,10,13,16,20,22,24}</span></p>
<p><span class="font8">=W1 • &nbsp;</span><span class="font17">+ 而</span><span class="font8">2 • %2 </span><span class="font17">+ 而</span><span class="font8">3 • %3 + …</span><span class="font17">=尸而/ •叼 </span><span class="font25">（9）</span></p>
<div><img src="pidinet_files/pidinet-28.jpg" alt="" style="width:5pt;height:20pt;">
</div><br clear="all">
<p><a name="bookmark79"></a><span class="font3">1</span></p>
<p><span class="font3">0.9</span></p>
<p><span class="font3">0.8</span></p>
<p><span class="font3">0.7</span></p>
<p><span class="font3">0.6</span></p>
<p><span class="font3">0.5</span></p>
<p><span class="font3">0.4</span></p>
<p><span class="font3">0.3</span></p>
<p><span class="font3">0.2</span></p>
<p><span class="font3">0.1</span></p>
<p><span class="font3">0</span></p>
<div>
<p><span class="font4" style="font-weight:bold;">NYUD</span></p><img src="pidinet_files/pidinet-29.jpg" alt="" style="width:181pt;height:176pt;">
<p><span class="font3">0 &nbsp;&nbsp;0.1 &nbsp;&nbsp;0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 &nbsp;&nbsp;&nbsp;1</span></p>
<p><span class="font4">Recall</span></p>
</div><br clear="all">
<p><a name="bookmark111"></a><span class="font24">Figure 13. Precision-Recall curves of our models and some competitors on NYUD dataset.</span></p>
<p><span class="font25">The RPDC is converted to a vanilla convolution with kernel size 5 x 5.</span></p>
<p><span class="font25" style="font-weight:bold;">Conversion in the Inference Phase. </span><span class="font25">After training, instead of saving the original weights </span><span class="font25" style="font-style:italic;">wi,</span><span class="font25"> we directly save the new set of weights </span><span class="font25" style="font-style:italic;">Wi.</span><span class="font25"> Therefore, during inference, all the convolutional operations are vanilla convolutions.</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark112"></a><span class="font25" style="font-weight:bold;">7.2. Precision-Recall Curves on NYUD Dataset</span></h4></li></ul>
<p><span class="font25">The Precision-Reall curves of our methods and other approaches on NYUD dataset </span><a href="#bookmark54"><span class="font25">[48]</span></a><span class="font25"> are shown in Fig.</span><a href="#bookmark111"><span class="font25"> 13.</span></a><span class="font25"> The comparedmethods includeRCF </span><a href="#bookmark18"><span class="font25">[31]</span></a><span class="font25">, SE </span><a href="#bookmark23"><span class="font25">[9]</span></a><span class="font25">, gPb+NG </span><a href="#bookmark74"><span class="font25">[14]</span></a><span class="font25">, gPb-UCM </span><a href="#bookmark53"><span class="font25">[1]</span></a><span class="font25"> and OEF </span><a href="#bookmark22"><span class="font25">[16]</span></a><span class="font25">.</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark113"></a><span class="font25" style="font-weight:bold;">7.3. Visualization</span></h4></li></ul>
<p><span class="font25" style="font-weight:bold;">Edge Maps. </span><span class="font25">The edge maps generated from the baseline architecture and PiDiNet are shown in Fig.</span><a href="#bookmark114"><span class="font25"> 14.</span></a><span class="font25"> Both models were trained using only the BSDS500 dataset without the mixed VOC dataset </span><a href="#bookmark56"><span class="font25">[40]</span></a><span class="font25">. From the figure, it is proved that PDC can help PiDiNet effectively capture more useful boundaries, with the ability to extract rich gradient information that facilitates edge detection.</span></p>
<p><span class="font25" style="font-weight:bold;">Intermediate Feature Maps. </span><span class="font25">We also visualize the intermediate feature maps extracted from PiDiNet, to qualitatively demonstrate the effectiveness of the compact dilation convolution based module (CDCM) and the compact spatial attention module (CSAM), which are shown in Fig.</span><a href="#bookmark115"><span class="font25"> 15.</span></a><span class="font25"> It is concluded that both CDCM and CSAM take a positive role in PiDiNet on the edge detection task.</span></p>
<div><img src="pidinet_files/pidinet-30.jpg" alt="" style="width:241pt;height:184pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-31.jpg" alt="" style="width:193pt;height:58pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-32.jpg" alt="" style="width:193pt;height:58pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-33.jpg" alt="" style="width:193pt;height:58pt;">
</div><br clear="all">
<div>
<p><a name="bookmark80"></a><span class="font3">Input</span></p><img src="pidinet_files/pidinet-34.jpg" alt="" style="width:87pt;height:58pt;">
</div><br clear="all">
<div>
<p><span class="font3">GT</span></p><img src="pidinet_files/pidinet-35.jpg" alt="" style="width:90pt;height:61pt;">
</div><br clear="all">
<div>
<p><span class="font3">Baseline with vanilla conv</span></p><img src="pidinet_files/pidinet-36.jpg" alt="" style="width:430pt;height:58pt;">
</div><br clear="all">
<div>
<p><span class="font3">PiDiNet with PDC</span></p><img src="pidinet_files/pidinet-37.jpg" alt="" style="width:430pt;height:52pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-38.jpg" alt="" style="width:478pt;height:184pt;">
<p><a name="bookmark114"></a><span class="font24">Figure 14. For each case, Top: input and ground truth image; Middle: edge maps from stage 1, 2, 3, 4 respectively and the final edge map, generated from the baseline architecture, Bottom: Corresponding edge maps generated from PiDiNet. Both the baseline architecture and PiDiNet were trained only using the BSDS500 dataset </span><a href="#bookmark53"><span class="font24">[1]</span></a><span class="font24">. Compared with the baseline, we can see that PiDiNet can detect more useful boundaries </span><span class="font17" style="font-style:italic;">(</span><span class="font24" style="font-style:italic;">e.g.</span><span class="font17" style="font-style:italic;">,</span><span class="font24"> bangs, stairs, the contour of the tree, the characteristic textures of the car).</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-39.jpg" alt="" style="width:492pt;height:131pt;">
</div><br clear="all">
<div><img src="pidinet_files/pidinet-40.jpg" alt="" style="width:274pt;height:129pt;">
<p><span class="font1">Input</span></p>
<p><span class="font1">Feature maps from the backbone</span></p>
<p><span class="font1">After CDCM</span></p>
<p><a name="bookmark115"></a><span class="font24">Figure 15. CDCM and CSAM can further refine the feature maps with multi-scale feature extraction and the sample adaptive spatial attention mechanism. Note that in the attention maps generated by CSAM, pixels in the background show higher intensities. This makes sense as the background pixels after CDCM have negative values, hence they will be additionally suppressed through CSAM.</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-41.jpg" alt="" style="width:138pt;height:128pt;">
<p><span class="font1">Attention map in CSAM</span></p>
<p><span class="font1">After CSAM</span></p>
</div><br clear="all">
<div><img src="pidinet_files/pidinet-42.jpg" alt="" style="width:58pt;height:42pt;">
</div><br clear="all">
<div>
<p><span class="font1">Output</span></p>
</div><br clear="all">
<p><sup><a href="#footnote1">1</a></sup><a name="bookmark45"></a></p>
<p><span class="font23"><sup></sup> Usually, higher activation responses are considered to be more salient, as adopted in many network pruning methods [ L7, 51]</span></p>
</body>
</html>